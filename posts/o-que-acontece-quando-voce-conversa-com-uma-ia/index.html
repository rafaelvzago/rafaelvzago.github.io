<!doctype html><html lang="pt-BR" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="theme-color" media="(prefers-color-scheme: light)" content="#f7f7f7"><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1b1b1e"><meta name="mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"><meta name="viewport" content="width=device-width, user-scalable=no initial-scale=1, shrink-to-fit=no, viewport-fit=cover" ><meta name="generator" content="Jekyll v4.4.1" /><meta property="og:title" content="O Que Acontece Quando Você Conversa com uma IA" /><meta property="og:locale" content="pt_BR" /><meta name="description" content="Conteúdo de cloud, devops, infraestrutura, automação, linux, docker, kubernetes, ansible, terraform, aws, azure, gcp, Skupper, OpenShift, Containers, Orquestração de contêineres, Microservices, Arquitetura de software, CI/CD pipelines, YAML (para configuração), Python (para automação), Go (para desenvolvimento), Red Hat, Cloud-Native, DevOps Best Practices, GitOps, Jenkins, Helm (para Kubernetes), Istio (para serviços de malha), Serverless, Prometheus (para monitoramento)" /><meta property="og:description" content="Conteúdo de cloud, devops, infraestrutura, automação, linux, docker, kubernetes, ansible, terraform, aws, azure, gcp, Skupper, OpenShift, Containers, Orquestração de contêineres, Microservices, Arquitetura de software, CI/CD pipelines, YAML (para configuração), Python (para automação), Go (para desenvolvimento), Red Hat, Cloud-Native, DevOps Best Practices, GitOps, Jenkins, Helm (para Kubernetes), Istio (para serviços de malha), Serverless, Prometheus (para monitoramento)" /><link rel="canonical" href="https://rafaelvzago.github.io/posts/o-que-acontece-quando-voce-conversa-com-uma-ia/" /><meta property="og:url" content="https://rafaelvzago.github.io/posts/o-que-acontece-quando-voce-conversa-com-uma-ia/" /><meta property="og:site_name" content="Rafael Zago" /><meta property="og:image" content="https://rafaelvzago.github.io/assets/img/headers/o-que-acontece-quando-voce-conversa-com-uma-ia.png" /><meta property="og:image:alt" content="O Que Acontece Quando Você Conversa com uma IA" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2025-12-24T00:00:00-03:00" /><meta name="twitter:card" content="summary_large_image" /><meta property="twitter:image" content="https://rafaelvzago.github.io/assets/img/headers/o-que-acontece-quando-voce-conversa-com-uma-ia.png" /><meta name="twitter:image:alt" content="O Que Acontece Quando Você Conversa com uma IA" /><meta property="twitter:title" content="O Que Acontece Quando Você Conversa com uma IA" /><meta name="twitter:site" content="@rafaelvzago" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2025-12-24T00:00:00-03:00","datePublished":"2025-12-24T00:00:00-03:00","description":"Conteúdo de cloud, devops, infraestrutura, automação, linux, docker, kubernetes, ansible, terraform, aws, azure, gcp, Skupper, OpenShift, Containers, Orquestração de contêineres, Microservices, Arquitetura de software, CI/CD pipelines, YAML (para configuração), Python (para automação), Go (para desenvolvimento), Red Hat, Cloud-Native, DevOps Best Practices, GitOps, Jenkins, Helm (para Kubernetes), Istio (para serviços de malha), Serverless, Prometheus (para monitoramento)","headline":"O Que Acontece Quando Você Conversa com uma IA","image":{"alt":"O Que Acontece Quando Você Conversa com uma IA","url":"https://rafaelvzago.github.io/assets/img/headers/o-que-acontece-quando-voce-conversa-com-uma-ia.png","@type":"imageObject"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://rafaelvzago.github.io/posts/o-que-acontece-quando-voce-conversa-com-uma-ia/"},"url":"https://rafaelvzago.github.io/posts/o-que-acontece-quando-voce-conversa-com-uma-ia/"}</script><title>O Que Acontece Quando Você Conversa com uma IA | Rafael Zago</title><link rel="icon" type="image/png" href="/assets/img/favicons/favicon-96x96.png" sizes="96x96"><link rel="icon" type="image/svg+xml" href="/assets/img/favicons/favicon.svg"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="/assets/css/jekyll-theme-chirpy.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@7.1.0/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/tocbot@4.36.4/dist/tocbot.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/loading-attribute-polyfill@2.1.1/dist/loading-attribute-polyfill.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/glightbox@3.3.0/dist/css/glightbox.min.css"> <script src="/assets/js/dist/theme.min.js"></script> <script defer src="https://cdn.jsdelivr.net/combine/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js,npm/loading-attribute-polyfill@2.1.1/dist/loading-attribute-polyfill.umd.min.js,npm/glightbox@3.3.0/dist/js/glightbox.min.js,npm/clipboard@2.0.11/dist/clipboard.min.js,npm/dayjs@1.11.18/dayjs.min.js,npm/dayjs@1.11.18/locale/pt.js,npm/dayjs@1.11.18/plugin/relativeTime.js,npm/dayjs@1.11.18/plugin/localizedFormat.js,npm/tocbot@4.36.4/dist/tocbot.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script defer src="/app.min.js?baseurl=&register=" ></script> <script defer src="https://www.googletagmanager.com/gtag/js?id=G-9J3YRPN8EN"></script> <script> document.addEventListener('DOMContentLoaded', () => { window.dataLayer = window.dataLayer || []; function gtag() { dataLayer.push(arguments); } gtag('js', new Date()); gtag('config', 'G-9J3YRPN8EN'); }); </script><body><aside aria-label="Sidebar" id="sidebar" class="d-flex flex-column align-items-end"><header class="profile-wrapper"> <a href="/" id="avatar" class="rounded-circle"><img src="https://gravatar.com/userimage/75121319/3dc8bb7e7befedea878842edafcb1335.jpeg?size=256" width="112" height="112" alt="avatar" onerror="this.style.display='none'"></a> <a class="site-title d-block" href="/">Rafael Zago</a><p class="site-subtitle fst-italic mb-0">Meu bloco de notas na interwebs..</p></header><nav class="flex-column flex-grow-1 w-100 ps-0"><ul class="nav"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream"></i> <span>CATEGORIAS</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tags"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive"></i> <span>ARQUIVOS</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle"></i> <span>SOBRE</span> </a></ul></nav><div class="sidebar-bottom d-flex flex-wrap align-items-center w-100"> <button type="button" class="btn btn-link nav-link" aria-label="Switch Mode" id="mode-toggle"> <i class="fas fa-adjust"></i> </button> <span class="icon-border"></span> <a href="https://github.com/rafaevzago" aria-label="github" target="_blank" rel="noopener noreferrer" > <i class="fab fa-github"></i> </a> <a href="https://twitter.com/rafaelvzago" aria-label="twitter" target="_blank" rel="noopener noreferrer" > <i class="fa-brands fa-x-twitter"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></aside><div id="main-wrapper" class="d-flex justify-content-center"><div class="container d-flex flex-column px-xxl-5"><header id="topbar-wrapper" class="flex-shrink-0" aria-label="Top Bar"><div id="topbar" class="d-flex align-items-center justify-content-between px-lg-3 h-100" ><nav id="breadcrumb" aria-label="Breadcrumb"> <span> <a href="/">Home</a> </span> <span>O Que Acontece Quando Você Conversa com uma IA</span></nav><button type="button" id="sidebar-trigger" class="btn btn-link" aria-label="Sidebar"> <i class="fas fa-bars fa-fw"></i> </button><div id="topbar-title"> Post</div><button type="button" id="search-trigger" class="btn btn-link" aria-label="Search"> <i class="fas fa-search fa-fw"></i> </button> <search id="search" class="align-items-center ms-3 ms-lg-0"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Buscar..." > </search> <button type="button" class="btn btn-link text-decoration-none" id="search-cancel">Cancelar</button></div></header><div class="row flex-grow-1"><main aria-label="Main Content" class="col-12 col-lg-11 col-xl-9 px-md-4"><article class="px-1" data-toc="true"><header><h1 data-toc-skip>O Que Acontece Quando Você Conversa com uma IA</h1><div class="post-meta text-muted"> <span> Postado em <time data-ts="1766545200" data-df="DD/MM/YYYY" data-bs-toggle="tooltip" data-bs-placement="bottom" > 24/12/2025 </time> </span><div class="mt-3 mb-3"> <a href="/assets/img/headers/o-que-acontece-quando-voce-conversa-com-uma-ia.png" class="popup img-link preview-img shimmer"><img src="/assets/img/headers/o-que-acontece-quando-voce-conversa-com-uma-ia.png" alt="O Que Acontece Quando Você Conversa com uma IA" width="1200" height="630" loading="lazy"></a><figcaption class="text-center pt-2 pb-2">O Que Acontece Quando Você Conversa com uma IA</figcaption></div><div class="d-flex justify-content-between"> <span> Por <em> <a href="https://github.com/rafaelvzago">Rafael Zago</a> </em> </span><div> <span class="readtime" data-bs-toggle="tooltip" data-bs-placement="bottom" title="1686 palavras" > <em>9 min</em> de leitura</span></div></div></div></header><div id="toc-bar" class="d-flex align-items-center justify-content-between invisible"> <span class="label text-truncate">O Que Acontece Quando Você Conversa com uma IA</span> <button type="button" class="toc-trigger btn me-1"> <i class="fa-solid fa-list-ul fa-fw"></i> </button></div><button id="toc-solo-trigger" type="button" class="toc-trigger btn btn-outline-secondary btn-sm"> <span class="label ps-2 pe-1">Conteúdo</span> <i class="fa-solid fa-angle-right fa-fw"></i> </button> <dialog id="toc-popup" class="p-0"><div class="header d-flex flex-row align-items-center justify-content-between"><div class="label text-truncate py-2 ms-4">O Que Acontece Quando Você Conversa com uma IA</div><button id="toc-popup-close" type="button" class="btn mx-1 my-1 opacity-75"> <i class="fas fa-close"></i> </button></div><div id="toc-popup-content" class="px-4 py-3 pb-4"></div></dialog><div class="content"> <iframe class="embed-video" loading="lazy" src="https://www.youtube.com/embed/uhxG2B96k7Q" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><h1 id="desvendando-os-modelos-de-linguagem-de-grande-escala">Desvendando os modelos de linguagem de grande escala</h1><p>Quando você digita uma mensagem pro ChatGPT ou pro Claude, o que acontece do outro lado é mais simples do que parece, mas opera numa escala difícil de visualizar.</p><h2 id="a-essência-um-narrador-de-futebol-digital"><span class="me-2">A essência: um narrador de futebol digital</span><a href="#a-essência-um-narrador-de-futebol-digital" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>Imagine a cena: final de Copa do Mundo, Brasil e Argentina, últimos minutos de jogo. O Galvão Bueno está narrando, a bola chega no atacante brasileiro, ele dribla um, dribla dois… e de repente o áudio falha. A transmissão continua, mas a voz do narrador desapareceu.</p><p>Agora imagine que você tem uma máquina capaz de analisar tudo que o Galvão disse até aquele momento, o tom, o ritmo, os bordões, o contexto do jogo, e prever qual seria a próxima palavra que ele diria. <em>“Vai que é tua…“</em>, <em>“Haja coração…“</em>, <em>“GOOOOOL…“</em>?</p><p>Com essa máquina, você poderia completar a narração palavra por palavra: alimentar o que já foi dito, receber uma previsão da próxima palavra mais provável, adicionar essa palavra à narração, e repetir até completar a transmissão inteira.</p><p>É basicamente isso que um LLM faz.</p><p>Um modelo de linguagem de grande escala é essencialmente uma função matemática sofisticada que recebe um texto como entrada e produz uma distribuição de probabilidades sobre todas as possíveis próximas palavras (ou, mais precisamente, tokens). É como ter milhares de narradores internos votando em qual seria a próxima palavra mais adequada.</p><p><a href="/assets/o-que-acontece-quando-voce-conversa-com-uma-ia-infografico.png" class="popup img-link shimmer"><img src="/assets/o-que-acontece-quando-voce-conversa-com-uma-ia-infografico.png" alt="Infográfico: Como funcionam os LLMs" loading="lazy"></a></p><h2 id="probabilidades-não-certezas"><span class="me-2">Probabilidades, não certezas</span><a href="#probabilidades-não-certezas" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>Vale notar: o modelo não “decide” qual palavra vem a seguir. Em vez disso, ele atribui uma probabilidade a cada palavra do seu vocabulário. Por exemplo:</p><div class="table-wrapper"><table><thead><tr><th>Próxima Palavra<th>Probabilidade<tbody><tr><td>“hoje”<td>32%<tr><td>“agora”<td>18%<tr><td>“aqui”<td>12%<tr><td>“sempre”<td>8%<tr><td>…<td>…</table></div><p>O sistema então seleciona uma palavra com base nessas probabilidades. Esse processo de amostragem é controlado por um parâmetro chamado <strong>temperatura</strong>:</p><ul><li><strong>Temperatura baixa</strong>: O modelo age como um narrador contido, tipo Milton Leite: escolhe as palavras mais prováveis e seguras, gerando respostas previsíveis e conservadoras<li><strong>Temperatura alta</strong>: O modelo vira um Galvão empolgado em final de Copa: aceita palavras menos prováveis e as respostas ficam mais criativas e emocionantes (mas potencialmente menos coerentes)</ul><p>Isso explica por que você pode fazer a mesma pergunta várias vezes e receber respostas diferentes. Dois narradores nunca descrevem o mesmo lance da mesma forma; o modelo introduz variação através desse processo de amostragem.</p><h2 id="o-treinamento-absorvendo-a-internet"><span class="me-2">O treinamento: absorvendo a internet</span><a href="#o-treinamento-absorvendo-a-internet" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>Como um modelo aprende a fazer essas previsões? A resposta está no volume enorme de dados de treinamento.</p><h3 id="a-escala-dos-dados"><span class="me-2">A escala dos dados</span><a href="#a-escala-dos-dados" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Para contextualizar: se você lesse 24 horas por dia, sem pausas, levaria aproximadamente <strong>2.600 anos</strong> para consumir a quantidade de texto usada no GPT-3. O GPT-4 utilizou um volume estimado em <strong>10 a 20 vezes maior</strong>. Modelos como Llama 3 da Meta foram treinados com mais de 15 trilhões de tokens.</p><p>O modelo processa todo esse texto através de um objetivo simples: dado um trecho de texto, prever qual é a próxima palavra. Parece trivial, mas esse objetivo força o modelo a desenvolver uma compreensão profunda da linguagem. O modelo acaba aprendendo:</p><ul><li>Gramática e sintaxe<li>Contexto e coerência<li>Conhecimento factual<li>Raciocínio lógico (até certo ponto)</ul><h3 id="parâmetros-os-controles-do-modelo"><span class="me-2">Parâmetros: os controles do modelo</span><a href="#parâmetros-os-controles-do-modelo" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Um LLM é definido por bilhões (às vezes trilhões) de valores numéricos chamados <strong>parâmetros</strong> ou <strong>pesos</strong>. Pense neles como os controles de uma mesa de som com bilhões de controles: cada ajuste altera sutilmente como o modelo se comporta.</p><p>O GPT-4 possui estimados <strong>1,7 trilhão de parâmetros</strong>. O Llama 3.1 da Meta tem versões de 8B, 70B e 405B parâmetros. O Claude 3 Opus tem cerca de 175 bilhões.</p><h3 id="retropropagação-aprendendo-com-os-erros"><span class="me-2">Retropropagação: aprendendo com os erros</span><a href="#retropropagação-aprendendo-com-os-erros" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Nenhum humano ajusta esses parâmetros manualmente. Eles começam com valores aleatórios (o modelo inicial produz apenas ruído) e são refinados através de um processo chamado <strong>retropropagação</strong> (backpropagation):</p><ol><li>O modelo recebe um texto de treinamento (menos a última palavra)<li>Faz uma previsão sobre qual seria a próxima palavra<li>Compara sua previsão com a palavra real<li>Um algoritmo ajusta todos os parâmetros para tornar o modelo <em>ligeiramente</em> mais propenso a acertar da próxima vez</ol><p><a href="/assets/llm-retropropagacao.png" class="popup img-link shimmer"><img src="/assets/llm-retropropagacao.png" alt="Fluxo de Retropropagação" loading="lazy"></a></p><p>Repita isso trilhões de vezes e o modelo começa a produzir previsões bem precisas, inclusive em textos que nunca viu.</p><h2 id="pré-treinamento-vs-fine-tuning-duas-etapas-distintas"><span class="me-2">Pré-treinamento vs. fine-tuning: duas etapas distintas</span><a href="#pré-treinamento-vs-fine-tuning-duas-etapas-distintas" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>O processo descrito acima é o <strong>pré-treinamento</strong>. Ele ensina o modelo a ser um excelente completador de texto, mas há um problema: completar texto da internet não é o mesmo que ser um assistente útil.</p><p>Um modelo pré-treinado pode continuar qualquer texto, mas não necessariamente vai responder perguntas de forma útil ou evitar conteúdo problemático.</p><h3 id="rlhf-aprendizado-por-reforço-com-feedback-humano"><span class="me-2">RLHF: aprendizado por reforço com feedback humano</span><a href="#rlhf-aprendizado-por-reforço-com-feedback-humano" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Para transformar um completador de texto em um assistente, os desenvolvedores aplicam uma segunda fase de treinamento chamada <strong>RLHF</strong> (Reinforcement Learning from Human Feedback):</p><ol><li>Humanos avaliam diferentes respostas do modelo para a mesma pergunta<li>Essas avaliações ensinam o modelo quais tipos de resposta são preferíveis<li>O modelo é ajustado para produzir mais respostas do tipo “preferível”</ol><p>Esse processo alinha o modelo com as expectativas humanas de utilidade, segurança e precisão.</p><h2 id="a-arquitetura-transformer-o-coração-dos-llms-modernos"><span class="me-2">A arquitetura Transformer: o coração dos LLMs modernos</span><a href="#a-arquitetura-transformer-o-coração-dos-llms-modernos" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>Antes de 2017, modelos de linguagem processavam texto sequencialmente, uma palavra por vez. Isso era lento e dificultava capturar dependências de longo alcance.</p><p>O paper “Attention Is All You Need” (Vaswani et al., 2017) introduziu a arquitetura <strong>Transformer</strong>, que revolucionou o campo.</p><h3 id="embeddings-traduzindo-palavras-em-números"><span class="me-2">Embeddings: traduzindo palavras em números</span><a href="#embeddings-traduzindo-palavras-em-números" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>O primeiro passo em qualquer modelo de linguagem é converter texto em números. Cada palavra (ou token) é representada por um vetor, uma lista de centenas ou milhares de números.</p><p>Esses vetores são chamados <strong>embeddings</strong> e codificam o “significado” das palavras de forma que palavras similares tenham vetores similares.</p><h3 id="o-mecanismo-de-atenção"><span class="me-2">O mecanismo de atenção</span><a href="#o-mecanismo-de-atenção" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>O grande diferencial dos Transformers é o mecanismo de <strong>atenção</strong> (attention). Em vez de processar palavras uma por vez, o modelo analisa todas as palavras simultaneamente e permite que cada palavra “consulte” todas as outras para refinar seu significado.</p><p>Por exemplo, na frase “O banco estava vazio”, a palavra “banco” precisa saber se estamos falando de uma instituição financeira ou um assento. O mecanismo de atenção permite que “banco” consulte “vazio” e outras palavras do contexto para resolver essa ambiguidade.</p><p>Matematicamente, a atenção calcula:</p><div class="language-plaintext highlighter-rouge"><div class="code-header"> <span data-label-text="Plaintext"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copiado!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>Attention(Q, K, V) = softmax(QK^T / √d_k) V
</pre></table></code></div></div><p>Onde Q (Query), K (Key) e V (Value) são projeções aprendidas dos embeddings de entrada.</p><h3 id="camadas-e-profundidade"><span class="me-2">Camadas e profundidade</span><a href="#camadas-e-profundidade" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Um Transformer típico empilha múltiplas camadas, cada uma contendo:</p><ol><li><strong>Multi-Head Attention</strong>: Múltiplos mecanismos de atenção em paralelo<li><strong>Feed-Forward Network</strong>: Uma rede neural densa que processa cada posição independentemente<li><strong>Normalização e Conexões Residuais</strong>: Técnicas para estabilizar o treinamento</ol><p>O GPT-4, por exemplo, possui estimadas 120 dessas camadas empilhadas, enquanto o Llama 3.1 405B tem 126 camadas.</p><h3 id="visualizando-a-arquitetura"><span class="me-2">Visualizando a arquitetura</span><a href="#visualizando-a-arquitetura" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p><a href="/assets/llm-transformer-arquitetura.png" class="popup img-link shimmer"><img src="/assets/llm-transformer-arquitetura.png" alt="Arquitetura do Transformer" loading="lazy"></a></p><p>O diagrama acima ilustra o fluxo de dados através de um Transformer. O texto entra, é convertido em embeddings, passa por múltiplas camadas de atenção e redes feed-forward, e finalmente produz uma distribuição de probabilidades sobre o vocabulário.</p><h2 id="a-escala-computacional-números-que-impressionam"><span class="me-2">A escala computacional: números que impressionam</span><a href="#a-escala-computacional-números-que-impressionam" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>Treinar um LLM moderno requer poder computacional absurdo. Para ilustrar:</p><p>Se você pudesse realizar um bilhão de operações matemáticas por segundo, quanto tempo levaria para executar todas as operações necessárias para treinar um dos maiores modelos de linguagem?</p><p>A resposta: <strong>mais de 100 milhões de anos</strong>.</p><p>Isso só é possível graças a:</p><ul><li><strong>GPUs</strong>: Chips especializados em processamento paralelo<li><strong>Clusters</strong>: Milhares de GPUs trabalhando em conjunto<li><strong>Paralelização</strong>: A arquitetura Transformer permite que grande parte do processamento ocorra simultaneamente</ul><p>O custo de treinar um modelo de ponta pode facilmente ultrapassar dezenas de milhões de dólares apenas em computação.</p><h2 id="testando-na-prática-com-python"><span class="me-2">Testando na prática com Python</span><a href="#testando-na-prática-com-python" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>Podemos explorar alguns desses conceitos com código. Aqui está um exemplo usando o <strong>Gemini</strong> do Google no <strong>Google Colab</strong>:</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copiado!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
</pre><td class="rouge-code"><pre><span class="c1"># Instalar/atualizar a biblioteca (Google Colab)
</span><span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="o">-</span><span class="n">q</span> <span class="o">-</span><span class="n">U</span> <span class="n">google</span><span class="o">-</span><span class="n">generativeai</span>

<span class="kn">import</span> <span class="n">google.generativeai</span> <span class="k">as</span> <span class="n">genai</span>
<span class="kn">from</span> <span class="n">google.colab</span> <span class="kn">import</span> <span class="n">userdata</span>

<span class="c1"># Configurar API key usando secret do Colab
</span><span class="n">genai</span><span class="p">.</span><span class="nf">configure</span><span class="p">(</span><span class="n">api_key</span><span class="o">=</span><span class="n">userdata</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="sh">'</span><span class="s">GEMINI_API_KEY</span><span class="sh">'</span><span class="p">))</span>

<span class="c1"># Usar modelo Gemini
</span><span class="n">model</span> <span class="o">=</span> <span class="n">genai</span><span class="p">.</span><span class="nc">GenerativeModel</span><span class="p">(</span><span class="sh">'</span><span class="s">gemini-2.0-flash</span><span class="sh">'</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">ver_probabilidades</span><span class="p">(</span><span class="n">texto</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Mostra possíveis continuações usando Gemini.</span><span class="sh">"""</span>

    <span class="n">prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"""</span><span class="s">Você é um especialista em modelos de linguagem. Dado o início da frase </span><span class="sh">"</span><span class="si">{</span><span class="n">texto</span><span class="si">}</span><span class="sh">"</span><span class="s">, liste as 5 palavras mais prováveis que um modelo de linguagem escolheria como próxima palavra.

Responda APENAS neste formato exato, com porcentagens estimadas:
palavra1 - XX%
palavra2 - XX%
palavra3 - XX%
palavra4 - XX%
palavra5 - XX%
</span><span class="sh">"""</span>

    <span class="n">response</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">generate_content</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>

    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Texto: </span><span class="sh">'</span><span class="si">{</span><span class="n">texto</span><span class="si">}</span><span class="sh">'"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="se">\n</span><span class="s">Próximas palavras mais prováveis:</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">-</span><span class="sh">"</span> <span class="o">*</span> <span class="mi">40</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">response</span><span class="p">.</span><span class="n">text</span><span class="p">)</span>

<span class="c1"># Exemplo de uso
</span><span class="nf">ver_probabilidades</span><span class="p">(</span><span class="sh">"</span><span class="s">O céu está</span><span class="sh">"</span><span class="p">)</span>
</pre></table></code></div></div><blockquote><p><strong>Configuração:</strong> No Google Colab, adicione sua API key do Gemini como um secret chamado <code class="language-plaintext highlighter-rouge">GEMINI_API_KEY</code>. Você pode gerar sua key em <a href="https://aistudio.google.com/app/apikey">aistudio.google.com/app/apikey</a>.</p></blockquote><p><strong>Output esperado:</strong></p><div class="language-plaintext highlighter-rouge"><div class="code-header"> <span data-label-text="Plaintext"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copiado!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre><td class="rouge-code"><pre>Texto: 'O céu está'

Próximas palavras mais prováveis:
----------------------------------------
azul - 35%
claro - 22%
nublado - 18%
lindo - 12%
escuro - 8%
</pre></table></code></div></div><p>Este código demonstra exatamente o que discutimos: o modelo analisa o contexto e estima quais palavras têm maior probabilidade de continuar a frase.</p><h2 id="conectando-com-outros-projetos"><span class="me-2">Conectando com outros projetos</span><a href="#conectando-com-outros-projetos" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>Se você quer rodar modelos localmente, veja meu post sobre <a href="/posts/running-local-ai-with-instruct-lab/">InstructLab e Skupper</a>, onde exploro como criar chatbots com dados protegidos usando conexões seguras entre diferentes ambientes.</p><h2 id="conclusão"><span class="me-2">Conclusão</span><a href="#conclusão" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>No fundo, LLMs são completadores de texto que aprenderam padrões a partir de volumes enormes de dados. Um objetivo simples (prever a próxima palavra), escala absurda (bilhões de parâmetros, trilhões de tokens), uma arquitetura eficiente (Transformer com atenção) e uma fase de alinhamento (RLHF). Junto, isso produz sistemas que parecem “entender” linguagem, embora estejam executando operações matemáticas em larga escala.</p><p>Entender esses fundamentos ajuda a usar melhor as ferramentas, identificar limitações (alucinações, vieses) e avaliar com mais realismo o que elas de fato fazem.</p><p>LLMs são ferramentas úteis. Não são mágica, não são inteligência geral. São matemática numa escala que é difícil de visualizar.</p><hr /><h2 id="referências"><span class="me-2">Referências</span><a href="#referências" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><ol><li>Vaswani, A. et al. (2017). <a href="https://arxiv.org/abs/1706.03762"><em>Attention Is All You Need</em></a>. NeurIPS.<li>Brown, T. et al. (2020). <a href="https://arxiv.org/abs/2005.14165"><em>Language Models are Few-Shot Learners</em></a>. NeurIPS.<li>Ouyang, L. et al. (2022). <a href="https://arxiv.org/abs/2203.02155"><em>Training language models to follow instructions with human feedback</em></a>. NeurIPS.<li>Radford, A. et al. (2019). <a href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf"><em>Language Models are Unsupervised Multitask Learners</em></a>. OpenAI.<li>3Blue1Brown. (2024). <a href="https://www.youtube.com/watch?v=LPZh9BOjkQs"><em>How LLMs Work</em></a>. YouTube.</ol></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw me-1"></i> <a href="/categories/ai/">ai</a>, <a href="/categories/mlops/">mlops</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw me-1"></i> <a href="/tags/ai/" class="post-tag no-text-decoration" >ai</a> <a href="/tags/llm/" class="post-tag no-text-decoration" >llm</a> <a href="/tags/machine-learning/" class="post-tag no-text-decoration" >machine-learning</a> <a href="/tags/transformers/" class="post-tag no-text-decoration" >transformers</a> <a href="/tags/deep-learning/" class="post-tag no-text-decoration" >deep-learning</a> <a href="/tags/chatbot/" class="post-tag no-text-decoration" >chatbot</a> <a href="/tags/gpt/" class="post-tag no-text-decoration" >gpt</a> <a href="/tags/tecnologia/" class="post-tag no-text-decoration" >tecnologia</a></div><div class=" post-tail-bottom d-flex justify-content-between align-items-center mt-5 pb-2 " ><div class="license-wrapper"> Esta postagem está licenciada sob <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> pelo autor.</div><div class="share-wrapper d-flex align-items-center"> <span class="share-label text-muted">Compartilhar</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=O%20Que%20Acontece%20Quando%20Voc%C3%AA%20Conversa%20com%20uma%20IA%20-%20Rafael%20Zago&url=https%3A%2F%2Frafaelvzago.github.io%2Fposts%2Fo-que-acontece-quando-voce-conversa-com-uma-ia%2F" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Twitter" aria-label="Twitter"> <i class="fa-fw fa-brands fa-square-x-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=O%20Que%20Acontece%20Quando%20Voc%C3%AA%20Conversa%20com%20uma%20IA%20-%20Rafael%20Zago&u=https%3A%2F%2Frafaelvzago.github.io%2Fposts%2Fo-que-acontece-quando-voce-conversa-com-uma-ia%2F" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Facebook" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https%3A%2F%2Frafaelvzago.github.io%2Fposts%2Fo-que-acontece-quando-voce-conversa-com-uma-ia%2F&text=O%20Que%20Acontece%20Quando%20Voc%C3%AA%20Conversa%20com%20uma%20IA%20-%20Rafael%20Zago" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Telegram" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <button id="copy-link" aria-label="Copy link" class="btn small" data-bs-toggle="tooltip" data-bs-placement="top" title="Copie o link" data-title-succeed="Link copiado com sucesso!" > <i class="fa-fw fas fa-link pe-none fs-6"></i> </button> </span></div></div></div></article></main><aside aria-label="Panel" id="panel-wrapper" class="col-xl-3 ps-2 text-muted"><div class="access"><section id="access-lastmod"><h2 class="panel-heading">Atualizados recentemente</h2><ul class="content list-unstyled ps-0 pb-1 ms-1 mt-2"><li class="text-truncate lh-lg"> <a href="/posts/o-que-acontece-quando-voce-conversa-com-uma-ia/">O Que Acontece Quando Você Conversa com uma IA</a><li class="text-truncate lh-lg"> <a href="/posts/openshift-service-mesh-3/">OpenShift Service Mesh 3.0</a><li class="text-truncate lh-lg"> <a href="/posts/dominando-pipelines-de-alta-performance/">Dominando Pipelines de Alta Performance: Do ClickOps ao GitOps com Jenkins e CasC</a><li class="text-truncate lh-lg"> <a href="/posts/controlando-progetendo-ia-deepseek-skupper-istio-terceiro-ultimo-ato/">Controlando e protegendo modelos de IA com segurança usando Deepseek, Skupper e InstructLab - Terceiro e Último Ato</a><li class="text-truncate lh-lg"> <a href="/posts/controlando-progetendo-ia-deepseek-skupper-istio-segundo-ato/">Controlando e protegendo modelos de IA com segurança usando Deepseek, Skupper e InstructLab - Segundo Ato</a></ul></section><section><h2 class="panel-heading">Trending Tags</h2><div class="d-flex flex-wrap mt-3 mb-1 me-3"> <a class="post-tag btn btn-outline-primary" href="/tags/skupper/">skupper</a> <a class="post-tag btn btn-outline-primary" href="/tags/ai/">ai</a> <a class="post-tag btn btn-outline-primary" href="/tags/devops/">devops</a> <a class="post-tag btn btn-outline-primary" href="/tags/kubernetes/">kubernetes</a> <a class="post-tag btn btn-outline-primary" href="/tags/networking/">networking</a> <a class="post-tag btn btn-outline-primary" href="/tags/redhat/">redhat</a> <a class="post-tag btn btn-outline-primary" href="/tags/instructlab/">instructlab</a> <a class="post-tag btn btn-outline-primary" href="/tags/openshift/">openshift</a> <a class="post-tag btn btn-outline-primary" href="/tags/security/">security</a> <a class="post-tag btn btn-outline-primary" href="/tags/llm/">llm</a></div></section></div><div class="toc-border-cover z-3"></div><section id="toc-wrapper" class="invisible position-sticky ps-0 pe-4 pb-4"><h2 class="panel-heading ps-3 pb-2 mb-0">Conteúdo</h2><nav id="toc"></nav></section></aside></div><div class="row"><div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 px-md-4"><aside id="related-posts" aria-labelledby="related-label"><h3 class="mb-4" id="related-label">Leia também</h3><nav class="row row-cols-1 row-cols-md-2 row-cols-xl-3 g-4 mb-4"><article class="col"> <a href="/posts/running-local-ai-with-instruct-lab/" class="post-preview card h-100"><div class="card-body"> <time data-ts="1722481200" data-df="DD/MM/YYYY" > 01/08/2024 </time><h4 class="pt-0 my-2">Running local AI with Instruct Lab and Skupper</h4><div class="text-muted"><p>Welcome to the Ollama Pilot. Problem to solve The main goal of this project is to create a secure connection between two sites, enabling the communication between the engineer machine and an In...</p></div></div></a></article><article class="col"> <a href="/posts/controlando-progetendo-ia-deepseek-skupper-istio-primeiro-ato/" class="post-preview card h-100"><div class="card-body"> <time data-ts="1746154800" data-df="DD/MM/YYYY" > 02/05/2025 </time><h4 class="pt-0 my-2">Controlando e protegendo modelos de IA com segurança usando Deepseek, Skupper e InstructLab - Primeiro Ato</h4><div class="text-muted"><p>Descubra como implantar e operar modelos de IA com segurança em ambientes híbridos, usando Skupper e InstructLab para proteger dados sensíveis.</p></div></div></a></article><article class="col"> <a href="/posts/AI-com-skupper-para-previnir-fraudes/" class="post-preview card h-100"><div class="card-body"> <time data-ts="1718593200" data-df="DD/MM/YYYY" > 17/06/2024 </time><h4 class="pt-0 my-2">Using Skupper and OpenShift AI/ML to Prevent Insurance Fraud</h4><div class="text-muted"><p>Description This workshop demonstrates how to use Skupper to connect local data services to cloud-based AI/ML environments. The workshop includes a Go application in a podman container that expose...</p></div></div></a></article></nav></aside><nav class="post-navigation d-flex justify-content-between" aria-label="Post Navigation"> <a href="/posts/openshift-service-mesh-3/" class="btn btn-outline-primary" aria-label="Anterior" ><p>OpenShift Service Mesh 3.0</p></a><div class="btn btn-outline-primary disabled" aria-label="Próximo"><p>-</p></div></nav><footer aria-label="Site Info" class=" d-flex flex-column justify-content-center text-muted flex-lg-row justify-content-lg-between align-items-lg-center pb-lg-3 " ><p>© <time>2026</time> <a href="https://github.com/rafaelvzago">Rafael Zago</a>. <span data-bs-toggle="tooltip" data-bs-placement="top" title="Exceto onde indicado de outra forma, as postagens do blog neste site são licenciadas sob a Creative Commons Attribution 4.0 International (CC BY 4.0) License pelo autor." >Alguns direitos reservados.</span></p><p>Feito com <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> usando o tema <a data-bs-toggle="tooltip" data-bs-placement="top" title="v7.4.1" href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener" >Chirpy</a></p></footer></div></div><div id="search-result-wrapper" class="d-flex justify-content-center d-none"><div class="col-11 content"><div id="search-hints"><section><h2 class="panel-heading">Trending Tags</h2><div class="d-flex flex-wrap mt-3 mb-1 me-3"> <a class="post-tag btn btn-outline-primary" href="/tags/skupper/">skupper</a> <a class="post-tag btn btn-outline-primary" href="/tags/ai/">ai</a> <a class="post-tag btn btn-outline-primary" href="/tags/devops/">devops</a> <a class="post-tag btn btn-outline-primary" href="/tags/kubernetes/">kubernetes</a> <a class="post-tag btn btn-outline-primary" href="/tags/networking/">networking</a> <a class="post-tag btn btn-outline-primary" href="/tags/redhat/">redhat</a> <a class="post-tag btn btn-outline-primary" href="/tags/instructlab/">instructlab</a> <a class="post-tag btn btn-outline-primary" href="/tags/openshift/">openshift</a> <a class="post-tag btn btn-outline-primary" href="/tags/security/">security</a> <a class="post-tag btn btn-outline-primary" href="/tags/llm/">llm</a></div></section></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><aside aria-label="Scroll to Top"> <button id="back-to-top" type="button" class="btn btn-lg btn-box-shadow"> <i class="fas fa-angle-up"></i> </button></aside></div><div id="mask" class="d-none position-fixed w-100 h-100 z-1"></div><aside id="notification" class="toast" role="alert" aria-live="assertive" aria-atomic="true" data-bs-animation="true" data-bs-autohide="false" ><div class="toast-header"> <button type="button" class="btn-close ms-auto" data-bs-dismiss="toast" aria-label="Close" ></button></div><div class="toast-body text-center pt-0"><p class="px-2 mb-3">Uma nova versão do conteúdo está disponível.</p><button type="button" class="btn btn-primary" aria-label="Update"> atualização </button></div></aside><script> document.addEventListener('DOMContentLoaded', () => { SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<article class="px-1 px-sm-2 px-lg-4 px-xl-0"><header><h2><a href="{url}">{title}</a></h2><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div></header><p>{content}</p></article>', noResultsText: '<p class="mt-5">Oops! Nenhum resultado encontrado.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="me-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); }); </script>
