[ { "title": "OpenShift Service Mesh 3.0", "url": "/posts/openshift-service-mesh-3/", "categories": "OpenShift, Service Mesh, Istio", "tags": "openshift, service-mesh, Istio, kiali, ambient-mode, devops, security, observability, redhat, migration", "date": "2025-07-07 00:00:00 -0300", "content": "Overview O OpenShift Service Mesh 3 (OSSM3) marca uma nova era na gestão de service meshes no ecossistema Red Hat, trazendo mudanças significativas e recursos avançados para ambientes corporativos. A principal novidade é a adoção do Istio como núcleo da solução, substituindo o Maistra e alinhando o OSSM3 com as tendências globais de service mesh. Entre os destaques, o OSSM3 oferece integração aprimorada com virtualização, suporte robusto a upgrades (tanto in-place quanto revisados), e uma experiência de gerenciamento mais rica com a inclusão do Kiali Console como operador padrão e um console dedicado para administração do mesh. A solução também amplia o suporte a ambientes multi-cluster, promovendo alta disponibilidade e resiliência. No âmbito do Istio, o controle passa a ser realizado em nível de cluster, proporcionando visibilidade global. Mudanças importantes incluem a remoção do gerenciamento de gateways pelo operador (agora feito via injeção por rota ou serviço), a descontinuação do Istio Operator Resource (IOR) e o fim do suporte à federação de meshes, exigindo contato direto com a Red Hat para necessidades específicas. Por fim, o OSSM3 traz o modo Istio Ambient Mode, com destaque para Zero Trust Tunnels (ztunnel), Waypoints (Envoy) para recursos avançados de camada 7 e a operação sidecarless, que reduz o consumo de recursos e simplifica a arquitetura. Essas inovações posicionam o OSSM3 como uma solução moderna, escalável e alinhada às demandas de segurança, observabilidade e flexibilidade das organizações que utilizam o OpenShift. Comparação entre OpenShift Service Mesh 2 e 3 OpenShift Service Mesh 2 OpenShift Service Mesh 3 Istioctl não é suportado Istioctl suportado - utilitários de diagnóstico Gateways e Rotas criados automaticamente Gateways e rotas criados pelos usuários Componentes de observabilidade gerenciados pelo operador do service mesh Componentes de observabilidade gerenciados independentemente Políticas de Rede do Kubernetes isolam uma service mesh por padrão Políticas de rede do Kubernetes não são criadas, podem ser definidas pelos usuários Múltiplos planos de controle por padrão Cluster-wide (abrangendo todo o cluster) por padrão Federação para múltiplos clusters Topologias multi-cluster do Istio + federação (em breve) Funcionalidades e Benefícios Foco em Segurança Aproveite segurança abrangente no networking de aplicações com criptografia mTLS transparente e políticas granulares, facilitando a implementação de redes zero trust. Gerenciamento de Tráfego Controle o fluxo de tráfego e chamadas de API entre serviços, tornando as comunicações mais confiáveis e a rede mais resiliente. Topologias Multicluster Implemente um único service mesh em múltiplos clusters OpenShift, garantindo alta disponibilidade entre clusters, zonas e regiões, com gestão unificada. Telemetria Compreenda as dependências entre serviços e o fluxo de tráfego via OpenShift Console, permitindo rápida identificação de problemas. Aplicação de Políticas Implemente políticas organizacionais nas interações entre serviços, garantindo o cumprimento de regras de acesso e distribuição justa de recursos. Observabilidade Visualize, valide e solucione problemas do service mesh com o OpenShift Service Mesh Console Plugin. Monitore a saúde dos componentes e inspecione traces e logs em uma interface unificada. Principais Mudanças e Recursos Istio substitui o Maistra A principal mudança do OSSM 3 em relação à versão 2.x é a adoção do Istio upstream como núcleo da solução, substituindo o Maistra (que era uma distribuição customizada do Istio). Essa transição alinha o OpenShift Service Mesh com o Istio padrão da comunidade, trazendo maior compatibilidade com o ecossistema Istio e acesso mais rápido a novos recursos e correções. Com o Istio upstream, o OSSM3 oferece uma base tecnológica mais alinhada com o projeto original, maior flexibilidade e suporte ampliado para integrações, além de simplificar operações e atualizações futuras. Estratégias de Upgrade O OSSM3 oferece duas estratégias de atualização do control plane do Istio, configuradas pelo campo upgradeStrategy: InPlace: Atualiza o control plane existente no cluster. Após a atualização, é necessário reiniciar os workloads para que passem a utilizar a nova versão do Istio. Essa abordagem é mais simples, porém envolve uma breve indisponibilidade durante o processo de reinício dos pods. RevisionBased (Canary): Cria um novo control plane Istio ao lado do existente, permitindo uma migração gradual dos workloads da versão antiga para a nova. Essa estratégia reduz riscos, pois possibilita validar a nova versão com parte dos workloads antes de migrar todo o ambiente, garantindo maior controle e segurança durante o upgrade. Exemplo de CR Istio para OSSM3 apiVersion: sailoperator.io/v1 kind: Istio metadata: name: default spec: version: v1.24.3 namespace: Istio-system updateStrategy: type: InPlace values: pilot: resources: requests: cpu: 100m memory: 1024Mi Observabilidade com Kiali Console O OSSM3 oferece suporte ao Kiali Console através de operador separado para observabilidade. É importante notar que o Kiali não é uma novidade exclusiva da v3, pois também era suportado na v2. De fato, na v2 o Kiali era instalado por padrão, enquanto na v3 precisa ser instalado separadamente. Essa mudança reflete a ideia de que o OSSM 3 oferece maior flexibilidade e pode ser integrado com uma ampla gama de soluções; o operador sail gerencia apenas o Istio. O Kiali proporciona: Visualização da Topologia: Interface gráfica intuitiva para mapear dependências entre serviços e compreender o fluxo de tráfego em tempo real. Métricas Integradas: Acesso direto às métricas do Istio com dashboards pré-configurados para monitoramento de performance e latência. Gestão de Configurações: Interface centralizada para validar e gerenciar políticas de tráfego, segurança e configurações do Istio. Troubleshooting Avançado: Ferramentas para identificar rapidamente problemas de conectividade, erros de configuração e gargalos de performance. Console Dedicado: Um console específico para administração do mesh, separado do OpenShift Console principal, permitindo foco exclusivo na gestão do service mesh. Com a instalação separada do Kiali Operator, os usuários têm maior controle sobre as funcionalidades de observabilidade, podendo configurar exatamente o que necessitam para seu ambiente específico. Request Authentication usando JWT O OSSM3 oferece suporte robusto para autenticação de requests baseada em JSON Web Tokens (JWT), permitindo validação segura de identidades em comunicações entre serviços. Esta funcionalidade é essencial para implementar arquiteturas zero trust e garantir que apenas requests autenticados acessem recursos protegidos. Recursos Principais Múltiplos Issuers: Suporte a diferentes provedores de JWT simultaneamente Validação JWKS: Recuperação automática de chaves públicas via JWKS URI Claims Customizados: Acesso a claims específicos para decisões de autorização Audience Validation: Verificação de audiência para garantir que tokens sejam destinados ao serviço correto Token Forwarding: Propagação segura de tokens JWT através do service mesh A implementação JWT no OSSM3 garante autenticação forte e flexível, integrando-se nativamente com provedores de identidade externos e oferecendo controle detalhado sobre pol��ticas de acesso. Operators OSSM3 Operator O operador do OpenShift Service Mesh 3 foi redesenhado com um escopo mais focado e especializado. Diferentemente das versões anteriores, o operador OSSM3 instala e gerencia exclusivamente o Istio, simplificando sua responsabilidade e melhorando a eficiência operacional. Kiali Operator para Observabilidade Para funcionalidades de observabilidade, o OSSM3 utiliza o Kiali Operator como componente separado e dedicado. Esta separação de responsabilidades oferece: Especialização: Cada operador foca em sua área específica de expertise Flexibilidade: Permite atualizações independentes do Kiali sem afetar o núcleo do Istio Modularidade: Facilita a manutenção e troubleshooting de componentes específicos Escalabilidade: Possibilita configurações personalizadas de observabilidade conforme necessidades do ambiente Esta arquitetura modular resulta em uma solução mais robusta, onde o operador OSSM3 mantém foco total no gerenciamento do Istio, enquanto o Kiali Operator oferece toda a stack de observabilidade necessária para monitoramento e análise do service mesh. Gateways no OpenShift Service Mesh 3 Um gateway é usado para gerenciar o tráfego que entra e sai do service mesh. Ele consiste em um proxy Envoy independente que é gerenciado pelo plano de controle do service mesh. Pode ser configurado usando um recurso Istio Gateway como: Um gateway de entrada (ingress) - um ponto de entrada para o mesh. Um gateway de saída (egress) - um ponto de saída do mesh. A partir do Service Mesh 2.6, também foi possível configurar gateways usando a API de Gateway do Kubernetes. Embora tecnicamente verdadeiro, em retrospectiva, a implementação era bastante imatura. Recomendamos fortemente que usuários interessados na API Gateway façam uso do OSSM v3 no OCP 4.19 ou superior, onde os CRDs são adequadamente gerenciados e suportados na plataforma OpenShift subjacente. No OpenShift Service Mesh 3, os gateways não são mais gerenciados pelo operador. Isso proporciona maior simplicidade, flexibilidade e incentiva a prática recomendada de gateways gerenciados juntamente com as aplicações. Os gateways podem ser criados com: Injeção de Gateway usando um Deployment do Kubernetes e expostos via: Um recurso Route do OpenShift. Um Service do Kubernetes do tipo LoadBalancer. Recursos da API de Gateway do Kubernetes. Escalabilidade &amp; Multi-Tenancy no OpenShift Service Mesh Antes de implementer o service mesh em múltiplos clusters, é importante considerar a motivação. As motivações podem incluir: Alta disponibilidade de serviços entre clusters, regiões, zonas, etc. Gerenciar políticas do Istio em múltiplos clusters a partir de um único plano de controle. Escalar políticas do service mesh em uma grande organização composta por múltiplas equipes. Gerenciar o compartilhamento de serviços entre clusters sem expô-los publicamente. Modelos de Multi-Cluster Modelo Descrição Características Multi-Primary Cada cluster possui um plano de controle Istio que gerencia tanto os serviços locais quanto os remotos. Maior disponibilidade.Mais configuração entre clusters e sincronização de estado. Primary-Remote Um único plano de controle gerencia toda a configuração, incluindo aquelas em clusters remotos. Sem redundância.Menos configuração entre clusters e sincronização de estado necessária. External Control Plane Para maior isolamento, o plano de controle pode ser implantado em um cluster completamente independente dos clusters do plano de dados. Isola os componentes de gerenciamento dos componentes do plano de dados.Suporta um modelo de cluster hub. Migrando para o Red Hat OpenShift Service Mesh 3 Existem várias coisas que os clientes podem fazer HOJE para se prepararem para a atualização para o Service Mesh 3: Atualizar para a versão mais recente disponível do OpenShift Service Mesh 2.6 Mover para a Injeção de Gateway (Gateway Injection) para criar e gerenciar todos os Gateways Desabilitar o IoR (Istio on Routes) e gerenciar explicitamente os Gateways com recursos de Rotas (Routes) Usar o monitoramento de projetos definido pelo usuário do OpenShift para métricas Migrar para OpenTelemetry e Tempo para rastreamento distribuído (distributed tracing) Configurar um recurso Kiali externo para gerenciar o Kiali Desabilitar o gerenciamento de políticas de rede (network policy) do OpenShift Service Mesh 2.6 Procedimentos detalhados para todos os itens acima fazem parte do guia de migração. Migrando cargas de trabalho (workloads) para o OpenShift Service Mesh 3 Os procedimentos de migração documentados visam mover as cargas de trabalho para o Red Hat OpenShift Service Mesh 3, mantendo a conectividade da aplicação. Os planos de controle do Service Mesh 2 e 3 são implantados em paralelo no mesmo namespace (por exemplo, Istio-system). Os rótulos (labels) das cargas de trabalho são então configurados para migrar para o plano de controle do Service Mesh 3 durante o próximo rollout. Istio Ambient Mode (Developer Preview) “Sidecar-less” service mesh Uma das maiores desvantagens dos service meshes tradicionais tem sido a exigência de que cada pod de aplicação tenha um proxy sidecar. Benefícios e Desafios dos proxies sidecar Envoy Benefícios Desafios Altamente customizável através das APIs do Istio, permitindo uma service mesh rica em funcionalidades. Com a flexibilidade do Envoy, vem a complexidade! Um modelo de implantação simples - um pod por proxy. Requer a modificação dos pods da aplicação para injetar os proxies. Uso de recursos eficiente quando gerenciado cuidadosamente. Um proxy por pod significa muitos proxies! Adição de latência tolerável para a maioria das aplicações voltadas para o usuário. Sem um gerenciamento cuidadoso, o uso de recursos pode sair do controle!   Impactos de performance para aplicações de baixa latência e alta vazão podem ser inaceitáveis.   Pode ser “pesado demais” se apenas algumas funcionalidades do mesh forem necessárias. Componentes do modo Ambient do Istio Istio-cni: É um daemonset que configura o redirecionamento do tráfego do pod com o ztunnel. Ztunnel: É um proxy por nó (per node) para lidar eficientemente com funcionalidades da Camada 4 (L4). Um proxy leve, de alta performance, “escrito em Rust”, e específico para o Istio. Habilita funcionalidades do mesh como criptografia mTLS, políticas de L4 e telemetria. (Opcional) Waypoint: É um proxy escalável de forma independente para funcionalidades da Camada 7 (L7). Habilita funcionalidades como políticas HTTP, telemetria e gerenciamento de tráfego. Um proxy Envoy, similar a um gateway. Implantado por namespace por padrão (modificado com labels). Escopo dos Proxies ZTunnel vs. Waypoint Os ZTunnels operam em um escopo por nó (per node). Isso significa que uma única instância do proxy ZTunnel é executada como um DaemonSet em cada nó de trabalho do cluster. Esse design é altamente eficiente, pois uma única instância leve pode gerenciar todo o tráfego da Camada 4 (L4) — como criptografia mTLS, autenticação e políticas de autorização básicas — para todos os pods agendados naquele nó específico. Ao centralizar a funcionalidade L4 no nível do nó, o modelo Ambient Mode reduz significativamente o consumo de recursos e a complexidade de gerenciamento em comparação com o modelo tradicional de um proxy sidecar para cada pod. Em contraste, os proxies Waypoint são, por padrão, implantados em um escopo por namespace ou, mais especificamente, por conta de serviço (service account). Quando funcionalidades avançadas da Camada 7 (L7), como balanceamento de carga, gerenciamento de tráfego complexo e políticas baseadas em HTTP, são necessárias para um serviço, um Waypoint proxy dedicado é implantado para aquele namespace. Esse proxy Envoy, que é mais robusto, intercepta o tráfego relevante e aplica as políticas L7 necessárias. Este modelo permite que as equipes ativem seletivamente as funcionalidades L7 apenas para as cargas de trabalho que precisam delas, isolando o consumo de recursos e a complexidade de configuração ao namespace correspondente, em vez de sobrecarregar todo o mesh. Túneis “Zero Trust“ (Ztunnels) Os proxies “ztunnel” rodam como um DaemonSet no nível do nó. Para pods que estão “no mesh” (in mesh), todo o tráfego (mesmo o tráfego local no nó) atravessa o proxy ztunnel local para que ele possa aplicar políticas e reportar telemetria. O diagrama comum do modo Ambient é uma simplificação do funcionamento do Ztunnel: A interceptação de tráfego ocorre dentro do namespace de rede do pod (e não no host). Isso garante que o tráfego não criptografado nunca saia do isolamento de rede do pod - exatamente como um sidecar! Até mesmo o tráfego local do nó será processado pelo ZTunnel. Conclusao O OpenShift Service Mesh 3 representa um avanço significativo na forma como as organizações gerenciam, protegem e observam suas aplicações baseadas em microserviços no OpenShift. A transição para o Istio como núcleo não apenas alinha a solução com o padrão de mercado, mas também oferece uma base mais sólida, flexível e preparada para o futuro. As melhorias na observabilidade com o Kiali, as estratégias de upgrade flexíveis e o gerenciamento simplificado de operadores e gateways capacitam as equipes de DevOps a operar com mais agilidade e segurança. A introdução do Istio Ambient Mode, mesmo em Developer Preview, sinaliza um futuro promissor com uma arquitetura “sidecar-less” que promete reduzir a sobrecarga de recursos e simplificar ainda mais a malha de serviços. Em suma, o OSSM3 é uma atualização crucial que fortalece o ecossistema do OpenShift, fornecendo as ferramentas necessárias para construir e operar aplicações resilientes, seguras e escaláveis, ao mesmo tempo em que estabelece um caminho claro para a inovação contínua no gerenciamento de service mesh. Referencias OpenShift Service Mesh Istio Kiali Argo Rollouts Envoy Proxy Kubernetes OpenTelemetry Grafana Tempo JSON Web Tokens (JWT) Rust" }, { "title": "Dominando Pipelines de Alta Performance: Do ClickOps ao GitOps com Jenkins e CasC", "url": "/posts/dominando-pipelines-de-alta-performance/", "categories": "DevOps, CI/CD, Jenkins, MLOps", "tags": "jenkins, cicd, devops, casc, gitops, automacao, mlops", "date": "2025-06-13 00:00:00 -0300", "content": "A Necessidade de Evoluir: Os Desafios do Jenkins “Básico” Jenkins é, sem dúvida, um dos servidores de automação mais poderosos e populares do mundo. Ele é o motor de inúmeros pipelines de CI/CD que compilam, testam e implantam aplicações. No entanto, quando gerenciado manualmente através da interface web — uma prática que muitos chamam de “ClickOps” — ele revela suas fraquezas. O “ClickOps” leva a desafios significativos que podem comprometer a agilidade e a confiabilidade de qualquer projeto, especialmente em ambientes complexos como os de MLOps: Falta de Versionamento e Auditoria: Quem alterou a configuração de um job? Por quê? Sem um controle de versão, essas perguntas são quase impossíveis de responder. Reverter uma mudança problemática se torna uma caça ao tesouro manual. Inconsistência e Difícil Reprodutibilidade: Replicar um ambiente Jenkins para desenvolvimento, homologação ou recuperação de desastres é uma tarefa árdua e propensa a erros. Pequenas diferenças de configuração entre ambientes podem causar falhas inesperadas. Escalabilidade e Manutenção Complexas: Gerenciar dezenas ou centenas de jobs manualmente é insustentável. A manutenção se torna um gargalo, e a padronização entre projetos, um sonho distante. Para alcançar a confiabilidade, velocidade, escalabilidade e segurança que os projetos modernos exigem, é preciso ir além do básico. Principais Tecnologias e Conceitos Jenkins é uma ferramenta de automação open-source escrita em Java que facilita a integração contínua e entrega contínua (CI/CD) para projetos de qualquer tamanho. Criado originalmente como “Hudson” em 2004 por Kohsuke Kawaguchi na Sun Microsystems, o Jenkins evoluiu para se tornar o servidor de automação mais popular do mundo, com mais de 300.000 instalações ativas e uma comunidade vibrante de desenvolvedores e usuários. Configuration as Code (CasC) representa uma evolução fundamental na forma como gerenciamos infraestrutura Jenkins. Esta abordagem permite definir toda a configuração do Jenkins - incluindo plugins, jobs, credenciais e configurações de sistema - através de arquivos YAML versionáveis. O plugin JCasC (Jenkins Configuration as Code) elimina a necessidade de configuração manual através da interface web, proporcionando reprodutibilidade, auditabilidade e facilidade de manutenção em ambientes complexos. GitOps é uma metodologia operacional que usa Git como fonte única da verdade para infraestrutura e configurações de aplicação. No contexto Jenkins, GitOps significa que todas as mudanças na configuração passam por um fluxo de Pull Request, permitindo revisão de código, testes automatizados e deployment controlado. Esta abordagem garante que o estado desejado do sistema esteja sempre sincronizado com o que está declarado no repositório Git, criando um ciclo de feedback contínuo e confiável para operações de infraestrutura. GitFlow e Times Distribuídos: Escalando a Colaboração Quando falamos de pipelines de alta performance em ambientes corporativos, não podemos ignorar a realidade dos times distribuídos e a necessidade de workflows organizados. É aqui que o GitFlow se torna essencial para gerenciar tanto o código da aplicação quanto a configuração do Jenkins. GitFlow: Organizando o Desenvolvimento O GitFlow é um modelo de branching que define papéis claros para diferentes tipos de branches: main/master: Código de produção, sempre estável develop: Branch de desenvolvimento, onde novas funcionalidades são integradas feature/*: Branches para desenvolvimento de funcionalidades específicas release/*: Preparação para releases hotfix/*: Correções urgentes em produção No contexto de CasC, aplicamos o mesmo modelo para gerenciar configurações Jenkins: # feature/new-python-pipeline jenkins: jobs: - script: | pipelineJob('python-ml-pipeline') { definition { cpsScm { scm { git('https://github.com/team/ml-project.git') } scriptPath('Jenkinsfile') } } } Desafios de Times Distribuídos Times distribuídos enfrentam desafios únicos que o CasC ajuda a resolver: Fusos Horários Diferentes: Com configuração como código, mudanças podem ser aplicadas automaticamente sem necessidade de coordenação síncrona entre equipes. Ambientes Locais Inconsistentes: Cada desenvolvedor pode ter uma instância Jenkins local idêntica à produção, eliminando o clássico “funciona na minha máquina”. Conhecimento Tribal: Documentar configurações em código elimina dependências de conhecimento específico de indivíduos, criando transparência para toda a equipe global. Estratégias para Colaboração Eficiente Pull Request Gates: Implemente aprovações obrigatórias de diferentes fusos horários: # .github/CODEOWNERS *.yaml @team-lead-americas @team-lead-europe @team-lead-asia Comunicação Assíncrona: Use PRs detalhados como forma de comunicação, explicando não apenas o “o quê” mas o “porquê” das mudanças de configuração. Rollback Automatizado: Configure pipelines que detectam falhas e fazem rollback automático para a última configuração estável, essencial quando não há cobertura 24/7. Jobs DSL: Programando Pipelines de Forma Declarativa Uma peça fundamental do ecossistema Jenkins para automação em escala é o Jobs DSL Plugin. Enquanto o CasC gerencia a configuração do sistema Jenkins, o Jobs DSL permite definir e gerenciar jobs de forma programática usando uma linguagem específica baseada em Groovy. Por que Jobs DSL é Essencial? Em vez de criar dezenas de jobs manualmente pela interface web, o Jobs DSL permite: Geração Massiva de Jobs: Criar múltiplos jobs similares com variações mínimas Padronização Automática: Garantir que todos os jobs sigam as mesmas convenções Manutenção Simplificada: Alterar um template e regenerar todos os jobs derivados Integração com CasC: Combinar configuração de sistema com definição de jobs Exemplo Prático: Pipeline para Múltiplos Projetos // Definindo uma lista de projetos de machine learning def mlProjects = [ [name: 'customer-churn', repo: 'ml-customer-churn', pythonVersion: '3.9'], [name: 'fraud-detection', repo: 'ml-fraud-detection', pythonVersion: '3.8'], [name: 'recommendation-engine', repo: 'ml-recommendations', pythonVersion: '3.10'] ] // Gerando um pipeline para cada projeto mlProjects.each { project -&gt; pipelineJob(\"ml-pipeline-${project.name}\") { description(\"Pipeline automatizado para ${project.name}\") definition { cpsScm { scm { git { remote { url(\"https://github.com/company/${project.repo}.git\") credentials('github-token') } branches('*/main') } } scriptPath('Jenkinsfile') } } properties { buildDiscarder { logRotator { numToKeepStr('10') artifactNumToKeepStr('5') } } } parameters { stringParam('PYTHON_VERSION', project.pythonVersion, 'Versão do Python para o projeto') booleanParam('DEPLOY_TO_STAGING', false, 'Deploy automático para staging?') } triggers { githubPush() cron('@daily') // Build diário para verificar dependências } } } Integrando Jobs DSL com CasC O Jobs DSL pode ser configurado através do CasC, criando um fluxo completamente automatizado: # jenkins.yaml - Configuração CasC jenkins: systemMessage: \"Jenkins com CasC + Jobs DSL\" jobs: - script: &gt; folder('ml-pipelines') { displayName('ML Pipelines') description('Pipelines automatizados para projetos de Machine Learning') } - script: &gt; pipelineJob('ml-pipelines/model-training-template') { definition { cps { script(''' pipeline { agent any parameters { choice(choices: ['xgboost', 'random-forest', 'neural-network'], name: 'MODEL_TYPE') string(name: 'DATASET_VERSION', defaultValue: 'latest') } stages { stage('Data Validation') { steps { sh 'python scripts/validate_data.py' } } stage('Model Training') { steps { sh \"python scripts/train_${params.MODEL_TYPE}.py\" } } stage('Model Evaluation') { steps { sh 'python scripts/evaluate_model.py' publishHTML([ allowMissing: false, alwaysLinkToLastBuild: true, keepAll: true, reportDir: 'reports', reportFiles: 'model_metrics.html', reportName: 'Model Metrics' ]) } } } post { always { archiveArtifacts artifacts: 'models/**', fingerprint: true junit 'test-results.xml' } } } ''') } } } Padrões Avançados com Jobs DSL Para ambientes complexos, considere estes padrões: // Template base para todos os projetos Python class PythonPipelineTemplate { static void create(job, config) { job.with { description(\"Pipeline Python para ${config.projectName}\") definition { cpsScm { scm { git(config.repoUrl) } scriptPath(config.jenkinsfile ?: 'Jenkinsfile') } } properties { buildDiscarder { logRotator(10, 5) } if (config.enableParameterizedTrigger) { parameters { stringParam('ENVIRONMENT', 'dev', 'Target environment') booleanParam('SKIP_TESTS', false, 'Skip test execution') } } } triggers { if (config.enableWebhook) githubPush() if (config.cronSchedule) cron(config.cronSchedule) } } } } // Uso do template def projectConfigs = [ [ projectName: 'data-preprocessing', repoUrl: 'https://github.com/company/data-preprocessing.git', enableWebhook: true, cronSchedule: 'H 2 * * *' ], [ projectName: 'model-serving', repoUrl: 'https://github.com/company/model-serving.git', enableWebhook: true, enableParameterizedTrigger: true ] ] projectConfigs.each { config -&gt; pipelineJob(\"python-${config.projectName}\") { PythonPipelineTemplate.create(delegate, config) } } Melhores Práticas para Jobs DSL Versionamento: Mantenha scripts DSL no mesmo repositório que outras configurações Modularização: Use templates e funções reutilizáveis Validação: Teste scripts DSL em ambiente de desenvolvimento primeiro Documentação: Comente templates complexos para facilitar manutenção Segurança: Use o modo sandbox sempre que possível A Revolução: Configuration as Code (CasC) A resposta para os desafios do “ClickOps” é o Configuration as Code (CasC). O conceito é simples, mas transformador: gerenciar TODA a configuração do seu Jenkins — desde configurações do sistema e plugins até os jobs e credenciais — como código, armazenado em um repositório Git. Os benefícios são imediatos e impactantes: Versionamento Total: Cada mudança na configuração é um commit no Git. Você sabe exatamente o que mudou, quem mudou e quando. Reverter uma alteração é tão simples quanto um git revert. Auditoria Clara: O histórico do Git serve como uma trilha de auditoria completa e imutável. Colaboração Eficiente: As mudanças na infraestrutura de CI/CD seguem o mesmo fluxo de trabalho que o código da aplicação: Pull Requests, code review e discussões em equipe. Automação e Reprodutibilidade: Com a configuração em código, você pode recriar uma instância Jenkins idêntica em minutos, de forma totalmente automatizada. Chega de inconsistências entre ambientes. NOTA: A abordagem CasC transforma a infraestrutura de CI/CD em um ativo de software, aplicando as mesmas boas práticas de desenvolvimento que usamos em nossas aplicações. Imagine definir a configuração do Jenkins com um simples arquivo YAML: jenkins: systemMessage: \"Jenkins gerenciado como código - Bem-vindo!\" numExecutors: 5 scm: - \"git\" security: globalJobDslSecurityConfiguration: useScriptSecurity: true tool: git: installations: - name: \"Default\" home: \"/usr/bin/git\" Este arquivo, uma vez aplicado, configura a instância Jenkins de forma determinística. Gerenciando CasC: Workflow, Segredos e Deployments Adotar CasC implica em um fluxo de trabalho estruturado. Uma mudança na configuração do Jenkins geralmente segue estes passos: Um desenvolvedor altera um arquivo de configuração .yaml em um branch. Um Pull Request (PR) é aberto para revisão. A equipe revisa a mudança. Após a aprovação e o merge, um pipeline automatizado aplica a nova configuração à instância Jenkins. Gerenciamento de Segredos: Um ponto crucial é o gerenciamento de segredos (senhas, tokens, chaves SSH). Eles NUNCA devem ser armazenados em texto puro no Git. Soluções como o HashiCorp Vault, AWS Secrets Manager ou as próprias credenciais do Jenkins integradas com o plugin CasC são essenciais para injetar segredos de forma segura. Pilares Essenciais: Monitoramento e Segurança Para sustentar um ambiente de alta performance, CasC deve ser complementado por monitoramento e segurança robustos. Monitoramento Monitorar seu ambiente Jenkins é vital para identificar gargalos e garantir a eficiência. Métricas chave incluem: Tempo de Build: Builds lentos atrasam o feedback para os desenvolvedores. Monitore para otimizar. Taxa de Sucesso: Quedas na taxa de sucesso podem indicar testes instáveis (“flaky tests”) ou problemas no ambiente. Uso de Executors: Garante que você tenha recursos suficientes para suas cargas de trabalho, evitando filas e atrasos. Segurança A segurança deve ser uma prioridade contínua: Princípio do Menor Privilégio: Conceda apenas as permissões necessárias para cada usuário ou sistema. Auditoria Contínua: Use o histórico do Git e os logs do Jenkins para monitorar atividades. Atualizações Constantes: Mantenha o Jenkins Core e todos os plugins sempre atualizados para se proteger contra vulnerabilidades. Segurança em Scripts Groovy: Scripts em Jenkinsfiles ou na Script Console são poderosos, mas podem ser um risco. Use o modo “sandbox” sempre que possível e valide os scripts rigorosamente. Conclusão Parabéns! Você explorou a jornada para transformar uma instância Jenkins tradicional em uma plataforma de automação moderna, robusta e escalável. Os principais pontos a lembrar são: CasC é fundamental: Abandonar o “ClickOps” em favor do Configuration as Code não é um luxo, mas uma necessidade para ambientes CI/CD e MLOps sérios. Otimize seus pipelines: A automação da configuração é o primeiro passo. O próximo é otimizar e proteger os próprios pipelines. Chamada para Ação: Comece pequeno. Pegue uma instância de desenvolvimento ou crie uma nova e comece a gerenciar uma pequena parte de sua configuração com o plugin CasC. A jornada para pipelines de alta performance começa com o primeiro commit. Referências e Recursos Adicionais Documentação Oficial Jenkins Configuration as Code Plugin - Plugin oficial para gerenciar Jenkins como código Documentação Oficial do Jenkins - Guia completo do Jenkins Jenkins Job DSL Plugin - Plugin para definir jobs programaticamente GitOps: What you need to know - Fundamentos da metodologia GitOps Guias e Tutoriais Avançados Jenkins Pipeline Documentation - Guia completo de pipelines declarativos e script Managing Jenkins with Configuration as Code - Projeto oficial JCasC GitFlow Workflow - Modelo de branching para times distribuídos Ferramentas de Segurança e Monitoramento HashiCorp Vault - Gerenciamento seguro de segredos Jenkins Monitoring with Prometheus - Plugin para métricas e monitoramento OWASP Jenkins Security Guidelines - Melhores práticas de segurança MLOps e CI/CD para Machine Learning MLOps with Jenkins - Soluções Jenkins para Machine Learning DVC - Data Version Control - Controle de versão para dados e modelos Kubeflow Pipelines - Pipelines ML em Kubernetes Comunidade e Suporte Jenkins Community Forum - Fórum oficial da comunidade Jenkins User Handbook - Manual do usuário Jenkins Awesome Jenkins - Lista curada de recursos Jenkins" }, { "title": "Controlando e protegendo modelos de IA com segurança usando Deepseek, Skupper e InstructLab - Terceiro e Último Ato", "url": "/posts/controlando-progetendo-ia-deepseek-skupper-istio-terceiro-ultimo-ato/", "categories": "AI, Kubernetes, instructlab", "tags": "ai, skupper, instructlab, kubernetes, nginx, ingress, loadbalancer", "date": "2025-05-05 00:00:00 -0300", "content": "Veja a Solução em Ação Neste artigo, vamos implementar passo a passo o chatbot InstructLab no Kubernetes usando Skupper para conectar de forma segura um modelo de IA privado com uma interface pública. Esta é a continuação prática do padrão de solução apresentado no artigo anterior. Conceitos e Comandos Usados na Demonstração NOTA: Os comandos a seguir são utilizados para configurar o ambiente e implantar o chatbot InstructLab. Eles são extraídos do projeto InstructLab e adaptados para esta demonstração. kubectl apply -f &lt;manifest&gt;: Aplica manifests YAML no cluster. kubectl create namespace &lt;namespace&gt;: Cria um novo namespace no Kubernetes. kubectl get pods -n &lt;namespace&gt;: Verifica os pods em um namespace específico. kubectl get services -n &lt;namespace&gt;: Lista os serviços em um namespace. kubectl port-forward service/&lt;service-name&gt; &lt;local-port&gt;:&lt;service-port&gt; -n &lt;namespace&gt;: Redireciona uma porta local para um serviço no cluster. Executar a demonstração Antes de começar Para configurar a demonstração, você precisa ter os seguintes pré-requisitos: Acesso a um cluster Kubernetes com o Skupper instalado. Um servidor executando o modelo de chat do InstructLab. Acesso a um terminal para executar os comandos. Acesso a um navegador da web para interagir com o chatbot. Cliente kubectl instalado e configurado para acessar o cluster Kubernetes. Cliente skupper instalado e configurado para acessar o cluster Kubernetes. Podman instalado para executar o Skupper privado. Implantando o Chatbot InstructLab Antes de executar o chatbot, vamos entender a parte final desta solução, o aplicativo Frontend. Este aplicativo será implantado em um cluster OpenShift e será responsável por enviar a entrada do usuário para o modelo de chat do InstructLab e exibir a resposta para o usuário. O aplicativo será implantado no mesmo namespace onde o Skupper público está em execução. Vamos agora implementar a aplicação diretamente no Kubernetes. O diretório manifests/ contém os recursos Kubernetes necessários: deployment.yaml - Deployment da aplicação com health checks service.yaml - Service ClusterIP para acesso interno ingress.yaml - Ingress NGINX para exposição externa loadbalancer-service.yaml - Service LoadBalancer para acesso externo Clonando o repositório Antes de começar a implantação, clone o repositório com os manifests necessários: # Clonar o repositório ilab-client git clone https://github.com/rafaelvzago/ilab-client.git cd ilab-client Instalando o NGINX Ingress Controller Primeiro, vamos instalar o NGINX Ingress Controller no cluster: # Instalar NGINX Ingress Controller kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.8.2/deploy/static/provider/cloud/deploy.yaml # Aguardar o controller estar pronto kubectl wait --namespace ingress-nginx \\ --for=condition=ready pod \\ --selector=app.kubernetes.io/component=controller \\ --timeout=120s # Verificar se o ingress controller está rodando kubectl get pods -n ingress-nginx kubectl get services -n ingress-nginx NOTA: O NGINX Ingress Controller é responsável por rotear o tráfego HTTP/HTTPS externo para os serviços dentro do cluster. Ele cria automaticamente um LoadBalancer service que expõe o cluster externamente. Implantando a aplicação # Implantar aplicação kubectl apply -f manifests/deployment.yaml -n ilab-chat kubectl apply -f manifests/service.yaml -n ilab-chat Configurando LoadBalancer Service Crie um arquivo loadbalancer-service.yaml ou aplique diretamente: apiVersion: v1 kind: Service metadata: name: ilab-client-lb namespace: ilab-chat labels: app: ilab-client spec: type: LoadBalancer ports: - port: 8080 targetPort: 8080 protocol: TCP name: http selector: app: ilab-client # Aplicar LoadBalancer service kubectl apply -f - &lt;&lt;EOF apiVersion: v1 kind: Service metadata: name: ilab-client-lb namespace: ilab-chat labels: app: ilab-client spec: type: LoadBalancer ports: - port: 8080 targetPort: 8080 protocol: TCP name: http selector: app: ilab-client EOF Configurando NGINX Ingress Crie um arquivo ingress.yaml ou aplique diretamente: apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: ilab-client-ingress namespace: ilab-chat annotations: kubernetes.io/ingress.class: nginx nginx.ingress.kubernetes.io/rewrite-target: / nginx.ingress.kubernetes.io/ssl-redirect: \"false\" spec: rules: - http: paths: - path: / pathType: Prefix backend: service: name: ilab-client port: number: 8080 # Aplicar Ingress NGINX kubectl apply -f - &lt;&lt;EOF apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: ilab-client-ingress namespace: ilab-chat annotations: kubernetes.io/ingress.class: nginx nginx.ingress.kubernetes.io/rewrite-target: / nginx.ingress.kubernetes.io/ssl-redirect: \"false\" spec: rules: - http: paths: - path: / pathType: Prefix backend: service: name: ilab-client port: number: 8080 EOF # Verificar deployment kubectl get pods -n ilab-chat kubectl get services -n ilab-chat kubectl get ingress -n ilab-chat NOTA: O comando kubectl apply -f manifests/deployment.yaml -n ilab-chat implanta o deployment da aplicação InstructLab no namespace ilab-chat. O deployment define como a aplicação será executada, incluindo o número de réplicas e as configurações do container. O comando kubectl apply -f manifests/service.yaml -n ilab-chat cria um serviço ClusterIP para a aplicação, permitindo que outros pods no cluster acessem a aplicação pelo nome do serviço. O LoadBalancer Service obtém um IP externo do provedor de nuvem (AWS ELB, GCP Load Balancer, Azure Load Balancer) para acesso direto à aplicação na porta 8080. O Ingress NGINX permite acesso HTTP/HTTPS à aplicação através do Ingress Controller, oferecendo recursos avançados como roteamento baseado em path, SSL/TLS termination, e balanceamento de carga. Verificando a implantação Para verificar se a aplicação foi implantada corretamente, vamos rodar um pod com o curl no namespace ilab-chat e fazer uma requisição para o serviço do InstructLab que vai ser roteado para o connector do Skupper que está rodando no site privado. # Rodar um pod temporário com curl kubectl run curl \\ --image=quay.io/skupper/lanyard \\ -n ilab-chat \\ --restart=Never \\ --rm \\ -i \\ --tty \\ -- \\ curl instructlab:8000 Saída esperada: {\"s{\"message\":\"Hello from InstructLab! Visit us at https://instructlab.ai\"}pod \"curl\" deleted NOTA: O comando kubectl run curl cria um pod temporário com a imagem quay.io/skupper/lanyard, que contém o utilitário curl. O pod é executado no namespace ilab-chat e é removido automaticamente após a execução (--rm). Acesso à aplicação Você tem três opções para acessar a aplicação implantada: Opção 1: Port-forward (desenvolvimento local) # Port-forward para a aplicação kubectl port-forward service/ilab-client 8080:8080 -n ilab-chat Agora você pode acessar a aplicação em http://localhost:8080. Opção 2: LoadBalancer Service (acesso direto) # Obter o IP externo do LoadBalancer kubectl get service ilab-client-lb -n ilab-chat # Aguardar até que EXTERNAL-IP não seja &lt;pending&gt; # Em seguida, acesse: http://&lt;EXTERNAL-IP&gt;:8080 Opção 3: NGINX Ingress (produção recomendada) # Obter o IP do Ingress Controller kubectl get service ingress-nginx-controller -n ingress-nginx # Verificar o status do Ingress kubectl get ingress ilab-client-ingress -n ilab-chat # Acessar via Ingress (substitua &lt;INGRESS-IP&gt; pelo IP obtido): # http://&lt;INGRESS-IP&gt;/ # ou configure DNS para apontar para o IP do Ingress NOTA: Port-forward: Ideal para desenvolvimento e testes locais. LoadBalancer: Fornece acesso direto com IP externo, ideal para ambientes simples. Ingress: Solução mais robusta para produção, permite configuração de SSL/TLS, múltiplos domínios e roteamento avançado. Conclusão Parabéns! Você implementou com sucesso um chatbot InstructLab no Kubernetes usando Skupper para conectar de forma segura um modelo de IA privado com uma interface pública. Esta solução permite que organizações mantenham seus modelos de IA em ambientes seguros e controlados, enquanto ainda fornecem acesso aos usuários finais através de uma interface web escalável. A combinação de InstructLab e Skupper fornece uma arquitetura robusta que atende aos requisitos de segurança e escalabilidade necessários para aplicações de IA empresariais. Referências: Comandos extraídos do projeto InstructLab Guia de instalação do InstructLab Chatbot ILAB Frontend NGINX Ingress Controller Kubernetes Ingress Kubernetes Services Skupper Documentation" }, { "title": "Controlando e protegendo modelos de IA com segurança usando Deepseek, Skupper e InstructLab - Segundo Ato", "url": "/posts/controlando-progetendo-ia-deepseek-skupper-istio-segundo-ato/", "categories": "AI, Kubernetes, instructlab", "tags": "ai, deepseek, skupper, instructlab, kubernetes, podman, llama-cpp, gguf", "date": "2025-05-04 00:00:00 -0300", "content": "Veja a Solução em Ação Neste artigo, vamos preparar todo o ambiente para servir o modelo de IA generativo Deepseek usando o InstructLab, baixando o modelo, convertendo-o para o formato GGUF e implantando o chatbot InstructLab. Além disso, vamos expor o serviço de forma segura usando Skupper. Conceitos e Comandos Usados na Demonstração NOTA Certifique-se de explicar o que cada comando Skupper faz na primeira vez que você o usa, especialmente para pessoas não familiarizadas com o Skupper. Os seguintes comandos devem ser explicados: skupper init: Inicializa a rede Skupper, configurando os componentes necessários para habilitar a comunicação segura entre os serviços. skupper expose: Expõe um serviço local através da rede Skupper, permitindo que ele seja acessado a partir de outros sites conectados ao Skupper. skupper token: Gera um token de conexão que pode ser usado por outros sites para se conectar à rede Skupper, garantindo uma comunicação segura. skupper link: Estabelece um link seguro entre dois sites Skupper usando o token criado por skupper token. skupper service status: Exibe o status dos serviços expostos através do Skupper, mostrando o que está acessível e como está conectado dentro da rede. ilab download: Baixa o modelo a ser usado pelo chatbot. ilab model serve: Inicia o servidor que será responsável por receber a entrada do usuário, enviá-la para o modelo LLaMA3 e enviar a resposta de volta ao usuário. ilab model chat: Inicia o chatbot, permitindo que o usuário interaja com ele. Executar a demonstração Antes de começar Para configurar a demonstração, você precisa ter os seguintes pré-requisitos: Acesso a um cluster Kubernetes com o Skupper e o InstructLab instalados. Um servidor executando o modelo de chat do InstructLab. Acesso a um terminal para executar os comandos. Acesso a um navegador da web para interagir com o chatbot. Cliente skupper instalado e configurado para acessar o cluster Kubernetes. Podman instalado para executar o Skupper privado. Implantação do Modelo de IA com o InstructLab O primeiro passo é implantar o modelo de chat do InstructLab no site do InstructLab. O modelo de chat do InstructLab será responsável por receber a entrada do usuário e enviá-la para o modelo LLaMA3. A resposta do modelo LLaMA3 será enviada de volta ao usuário. Isso é baseado no artigo: https://developers.redhat.com/blog/2024/06/12/getting-started-instructlab-generative-ai-model-tuning#model_alignment_and_training_with_instructlab [Primeiros passos com o InstructLab para ajuste de modelo de IA generativo]. mkdir instructlab &amp;&amp; cd instructlab sudo dnf install gcc gcc-c++ make git python3.11 python3.11-devel python3.11 -m venv --upgrade-deps venv source venv/bin/activate pip install instructlab NOTA O comando mkdir instructlab &amp;&amp; cd instructlab é usado para criar um diretório chamado instructlab e navegar até ele. O comando sudo dnf install gcc gcc-c++ make git python3.11 python3.11-devel é usado para instalar as dependências necessárias para o InstructLab. O comando python3.11 -m venv --upgrade-deps venv é usado para criar um ambiente virtual chamado venv para o InstructLab. O comando source venv/bin/activate é usado para ativar o ambiente virtual. O comando pip install instructlab é usado para instalar o InstructLab no ambiente virtual. Inicializar a Configuração do InstructLab Agora é hora de inicializar a configuração do InstructLab. O comando ilab config init é usado para inicializar a configuração do InstructLab, criando o arquivo config.yaml com a configuração padrão. Execute o seguinte comando: ilab config init A saída será semelhante à seguinte, com o usuário sendo solicitado a fornecer os valores necessários para inicializar o ambiente: Welcome to InstructLab CLI. This guide will help you to setup your environment. Please provide the following values to initiate the environment [press Enter for defaults]: Path to taxonomy repo [/home/user/.local/share/instructlab/taxonomy]: Path to your model [/home/user/.cache/instructlab/models/merlinite-7b-lab-Q4_K_M.gguf]: Generating `/home/user/.config/instructlab/config.yaml` and `/home/user/.local/share/instructlab/internal/train_configuration/profiles`... Please choose a train profile to use. Train profiles assist with the complexity of configuring specific GPU hardware with the InstructLab Training library. You can still take advantage of hardware acceleration for training even if your hardware is not listed. [0] No profile (CPU, Apple Metal, AMD ROCm) [1] Nvidia A100\\/H100 x2 (A100_H100_x2.yaml) [2] Nvidia A100\\/H100 x4 (A100_H100_x4.yaml) [3] Nvidia A100\\/H100 x8 (A100_H100_x8.yaml) [4] Nvidia L40 x4 (L40_x4.yaml) [5] Nvidia L40 x8 (L40_x8.yaml) [6] Nvidia L4 x8 (L4_x8.yaml) Enter the number of your choice [hit enter for no profile] [0]: No profile selected - any hardware acceleration for training must be configured manually. Initialization completed successfully, you're ready to start using `ilab`. Enjoy! NOTA O comando ilab config init é usado para inicializar a configuração do InstructLab, criando o arquivo config.yaml com a configuração padrão. O usuário é solicitado a fornecer os valores necessários para inicializar o ambiente, como o caminho para o repositório de taxonomia e o caminho para o modelo. Após executar o ilab config init, seus diretórios serão semelhantes aos seguintes em um sistema Linux: Arquivos e diretórios criados ~/.cache/instructlab/models/ # Diretório onde os modelos são armazenados. ~/.local/share/instructlab/datasets # Diretório onde os conjuntos de dados são armazenados. ~/.local/share/instructlab/taxonomy # Diretório onde a taxonomia é armazenada. ~/.local/share/instructlab/checkpoints # Diretório onde os checkpoints são armazenados. Para habilitar o acesso externo ao seu modelo, modifique o arquivo config.yaml, localizado no seu diretório instructlab: ~/.config/instructlab/config.yaml. Essa alteração precisa ser feita na seção serve, conforme mostrado abaixo: host_port: 0.0.0.0:8000 NOTA O host_port: O endereço IP e a porta onde o modelo será exposto. Neste caso, o modelo será exposto em todas as interfaces. Baixando e Convertendo o modelo DEEPSEEK para o InstructLab Agora, você precisa baixar o modelo que será usado pelo chatbot e convertê-lo para o formato GGUF. O InstructLab suporta vários modelos, mas para esta demonstração, usaremos o modelo DEEPSEEK. O comando ilab model download é usado para baixar o modelo a ser usado pelo chatbot. Criando um token do hugginface Para baixar o modelo, você precisa de um token do Hugging Face. Se você não tiver um token, crie uma conta no Hugging Face e gere um token de acesso. Depois de obter o token, execute o seguinte comando para configurá-lo: export HF_TOKEN=&lt;seu_token&gt; NOTA O comando export HF_TOKEN=&lt;seu_token&gt; é usado para definir a variável de ambiente HF_TOKEN com o seu token do Hugging Face. Isso é necessário para baixar o modelo do Hugging Face. Baixando o modelo DEEPSEEK Antes de iniciar o servidor, baixe o modelo a ser usado pelo chatbot. O comando ilab download é usado para baixar o modelo a ser usado pelo chatbot. Execute o seguinte comando: ilab model download --repository deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B NOTA O comando ilab download é usado para baixar o modelo a ser usado pelo chatbot. Convertendo o modelo para formato GGUF Após baixar o modelo, você precisa convertê-lo para o formato GGUF usando o llama.cpp. Primeiro, clone o repositório llama.cpp: git clone https://github.com/ggerganov/llama.cpp cd llama.cpp Siga as instruções de instalação do llama.cpp conforme descrito no repositório. Após a instalação, execute o comando de conversão: python ./convert_hf_to_gguf.py /home/rzago/.cache/instructlab/models/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B --outfile deepseek_f16.gguf --outtype f16 Agora mova o arquivo convertido para o diretório de modelos do InstructLab: mv deepseek_f16.gguf $HOME/.cache/instructlab/models/DeepSeek-R1-Distill-Qwen-1.5B.gguf NOTA O comando git clone https://github.com/ggerganov/llama.cpp é usado para baixar o repositório llama.cpp. O comando python ./convert_hf_to_gguf.py é usado para converter o modelo baixado do formato Hugging Face para o formato GGUF. O parâmetro --outtype f16 especifica que a conversão será feita em formato float16 para otimizar o tamanho do arquivo. O comando mv move o arquivo convertido para o diretório padrão onde o InstructLab procura por modelos. Agora, inicie o servidor que será responsável por receber a entrada do usuário, enviá-la para o modelo e enviar a resposta de volta ao usuário. O comando ilab model serve é usado para iniciar o servidor. Execute o seguinte comando: ilab model serve --model-path ~/.cache/instructlab/models/DeepSeek-R1-Distill-Qwen-1.5B.gguf NOTA O comando ilab model serve --model-path é usado para iniciar o servidor especificando o caminho do modelo convertido que será responsável por receber a entrada do usuário, enviá-la para o modelo e enviar a resposta de volta ao usuário. Implantação do Skupper Público Para expor o serviço do InstructLab para a internet, você precisa implantar o Skupper público no Ambiente Local Público. O Skupper público será responsável por expor o serviço do InstructLab para a internet, permitindo que o aplicativo Ollama Pilot envie solicitações ao modelo de chat do InstructLab. Nessa demonstração, estamos utilizando o Kubernetes como plataforma para implantar o Skupper público. O Skupper público será responsável por expor o serviço do InstructLab para a internet, permitindo que o aplicativo Ollama Pilot envie solicitações ao modelo de chat do InstructLab. Para instalar o skupper, vamos usar o skupper cli, que é uma ferramenta de linha de comando para interagir com o Skupper. Instalar o Skupper Público Para instalar o Skupper público, vamos usar o skupper cli, que é uma ferramenta de linha de comando para interagir com o Skupper. O Skupper CLI é usado para criar e gerenciar sites Skupper, além de expor serviços e estabelecer links entre sites. Instalar o Skupper CLI Execute o seguinte comando para instalar o Skupper CLI: curl https://skupper.io/v2/install.sh | sh NOTA O comando curl https://skupper.io/v2/install.sh | sh é usado para baixar e instalar o Skupper CLI no seu ambiente local. Isso é necessário para interagir com o Skupper e implantar o serviço do InstructLab. Instalar o Skupper no cluster Kubernetes Após instalar o Skupper CLI, você precisa instalar o Skupper no cluster Kubernetes. Execute o seguinte comando para instalar o Skupper no cluster Kubernetes: kubectl create ns skupper kubectl apply -f https://skupper.io/v2/install.yaml -n skupper NOTA O comando kubectl create ns skupper cria um namespace chamado skupper no cluster Kubernetes. Isso é necessário para isolar os recursos do Skupper e do InstructLab em um namespace separado. O comando kubectl apply -f https://skupper.io/v2/install.yaml -n skupper aplica o manifesto de instalação do Skupper no cluster Kubernetes, criando os recursos necessários para o Skupper funcionar. Isso inclui o pod skupper-router, que é responsável pelo roteamento de tráfego entre sites. Criar o namespace e inicializar o Skupper no cluster Kubernetes Após instalar o Skupper CLI, você precisa criar um namespace no cluster Kubernetes onde o Skupper será implantado. Execute o seguinte comando para criar um namespace chamado ilab-chat e inicializar o Skupper: kubectl create ns ilab-chat export SKUPPER_PLATFORM=kubernetes skupper site create ilab-chat -n ilab-chat --enable-link-access NOTA O comando kubectl create ns ilab-chat cria um novo namespace chamado ilab-chat no cluster Kubernetes. Isso é necessário para isolar os recursos do Skupper e do InstructLab em um namespace separado. O comando export SKUPPER_PLATFORM=kubernetes define a plataforma como Kubernetes, que é necessária para o Skupper funcionar corretamente. O comando skupper site create ilab-chat -n ilab-chat --enable-link-access inicializa o Skupper no namespace ilab-chat, criando os recursos necessários para a comunicação segura entre os serviços. A opção --enable-link-access permite que outros sites Skupper se conectem a este site, facilitando a comunicação entre diferentes ambientes. Verificar a instalação do Skupper Após a criação do namespace, você pode verificar se o Skupper foi instalado corretamente executando o seguinte comando: kubectl get pods -n ilab-chat A saída deve mostrar que o pod skupper-router está em execução, o que indica que o Skupper foi instalado com sucesso. NAME READY STATUS RESTARTS AGE skupper-router-cfc4c58f-pmhqv 2/2 Running 0 47s NOTA O comando kubectl get pods -n ilab-chat é usado para verificar o status dos pods no namespace ilab-chat. A saída deve mostrar que o pod skupper-router está em execução, o que indica que o Skupper foi instalado com sucesso. O pod skupper-router é o componente principal do Skupper, responsável pelo roteamento de tráfego entre sites. Implantação do Skupper Privado O segundo passo é implantar o Skupper privado no Ambiente Local Privado. O Skupper privado será responsável por criar uma conexão segura entre os dois sites, permitindo que o aplicativo Ollama Pilot envie solicitações ao modelo de chat do InstructLab. Instalar o Skupper Para instalar o skupper no site A, com o podman como plataforma, abra um novo terminal para lidar com todos os comandos relacionados ao Skupper privado. Aqui, criaremos um site Skupper usando o podman como plataforma, precisamos habilitar o serviço podman antes de executar o comando skupper init: systemctl --user enable --now podman.socket NOTA systemctl --user enable --now podman.socket é usado para habilitar e iniciar o serviço podman no nível do usuário. Agora, execute os seguintes comandos para instalar o Skupper: export SKUPPER_PLATFORM=podman skupper site create ilab-podman skupper connector create instructlab 8000 --host localhost NOTA SKUPPER_PLATFORM=podman é usado para definir a plataforma como podman. Isso é necessário porque o Skupper privado será executado em um contêiner podman. skupper site create ilab-podman é usado para criar um site Skupper chamado ilab-podman, configurando os componentes necessários para habilitar a comunicação segura entre os serviços. skupper connector create instructlab 8000 --host localhost é usado para criar um conector que conecta o serviço InstructLab local (executando na porta 8000) à rede Skupper, tornando-o disponível para sites remotos. Comunicação Segura Entre os Dois Sites com Skupper Agora é hora de estabelecer uma conexão segura entre os dois sites. Primeiro, no site público (Kubernetes), gere o link para conexão: skupper link generate -n ilab-chat &gt; /tmp/link.yaml NOTA skupper link generate é usado para gerar um arquivo de configuração de link que contém as credenciais necessárias para estabelecer uma conexão segura entre sites Skupper. Agora, no site privado (podman), prepare o diretório de recursos e mova o arquivo de link: mkdir -p $HOME/.local/share/skupper/namespaces/default/input/resources mv /tmp/link.yaml $HOME/.local/share/skupper/namespaces/default/input/resources/link.yaml skupper system setup --force NOTA O comando mkdir -p cria a estrutura de diretórios necessária para o Skupper gerenciar recursos de entrada. O arquivo link.yaml é movido para o diretório de recursos onde o Skupper pode detectá-lo automaticamente. skupper system setup --force força a reconfiguração do sistema Skupper para aplicar o novo link. Verifique o status do link Skupper: skupper link status link-ilab-chat Name: link-ilab-chat Status: Ok Cost: 1 TlsCredentials: link-ilab-chat Endpoint: skupper-router-inter-router-ilab-chat.apps.*********.com:443 NOTA skupper link status é usado para exibir o status de um link específico na rede Skupper. O status “Ok” indica que a conexão foi estabelecida com sucesso. O endpoint mostra a URL pública onde o link está ativo (mascarado por segurança). NOTA O parâmetro instructlab é o nome do serviço que será exposto, e 8000 é a porta onde o serviço está sendo executado. O parâmetro --host localhost especifica que o conector deve se conectar ao serviço local na interface de loopback (localhost), permitindo que o serviço seja acessado dentro do ambiente privado. Criando o Listener no Site Público A última etapa é criar um listener no site público para receber as conexões do connector no site privado. Esta é a configuração que permite que o aplicativo no cluster público acesse o modelo de IA no ambiente privado. Ainda no terminal onde o Skupper público está em execução, execute o seguinte comando para criar o listener, já que é neste site o serviço será consumido: skupper listener create instructlab 8000 -n ilab-chat NOTA O comando skupper listener create instructlab 8000 cria um listener que expõe o serviço instructlab na porta 8000 do site público, permitindo que ele seja acessado por outros sites conectados ao Skupper. O listener atua como um ponto de entrada na rede Skupper, recebendo conexões de outros sites e encaminhando-as para o serviço instructlab no site privado. Relacionamento Connector e Listener no Skupper: Listener (site público): skupper listener create instructlab 8000 - cria um ponto de entrada na rede Skupper para receber conexões O connector “empurra” o serviço local para a rede Skupper, enquanto o listener “puxa” esse serviço para o site remoto. Juntos, eles criam um túnel seguro que permite que aplicações no site público acessem serviços no site privado como se fossem locais. Iniciando o Serviço do Modelo no Site Privado Agora é hora de iniciar o serviço no terminal onde o site privado está sendo executado. Este é o passo final para ativar o modelo de IA e torná-lo disponível para receber requisições através da rede Skupper. Configurando o Servidor para Acesso Externo Antes de iniciar o servidor, você precisa modificar o arquivo de configuração do InstructLab para permitir acesso externo. Por padrão, o InstructLab serve apenas na interface local (127.0.0.1), mas para funcionar com o Skupper, precisamos configurá-lo para aceitar conexões de qualquer interface. Edite o arquivo de configuração localizado em ~/.config/instructlab/config.yaml e modifique a seção server: # Server configuration including host and port. ... host: 0.0.0.0 # Port to serve on. # Default: 8000 port: 8000 NOTA host: 0.0.0.0: Esta configuração permite que o servidor aceite conexões de todas as interfaces de rede, não apenas da interface local (localhost). Isso é essencial para que o Skupper consiga acessar o serviço. port: 8000: Mantém a porta padrão 8000, que corresponde à configuração do connector Skupper criado anteriormente. backend_type: llama-cpp: Especifica que será usado o backend llama-cpp para servir o modelo. Com essa configuração, o servidor estará pronto para receber conexões através da rede Skupper. No terminal onde você configurou o Skupper privado (podman), certifique-se de que o ambiente virtual do InstructLab esteja ativado e execute o seguinte comando: cd instructlab source venv/bin/activate ilab model serve --model-path ~/.cache/instructlab/models/DeepSeek-R1-Distill-Qwen-1.5B.gguf NOTA ilab model serve: Este comando inicia o servidor que será responsável por receber a entrada do usuário, enviá-la para o modelo DeepSeek e enviar a resposta de volta ao usuário. --model-path: Especifica o caminho para o modelo GGUF que será carregado pelo servidor. O modelo ficará disponível na porta 8000 (configurada anteriormente no arquivo config.yaml) e será acessível através da rede Skupper que conecta os dois sites. Após executar este comando, você verá uma saída similar a esta indicando que o servidor está funcionando: INFO 2025-01-15 10:30:45,123 Starting server on http://0.0.0.0:8000 INFO 2025-01-15 10:30:45,124 Model loaded successfully INFO 2025-01-15 10:30:45,125 Server ready to accept connections Agora o modelo DeepSeek está rodando no ambiente privado e disponível através da conexão segura Skupper para aplicações no site público. Testando a Conectividade através do Skupper do Site Público Agora que temos o modelo de IA rodando no site privado e a conexão Skupper estabelecida, vamos testar se a comunicação está funcionando corretamente a partir do site público (OpenShift). Para testar a conectividade de dentro do cluster Kubernetes, vamos criar um pod temporário que pode fazer requisições HTTP para o serviço do InstructLab: kubectl run curl --image=quay.io/skupper/lanyard -n ilab-chat --restart=Never --rm -i --tty -- curl instructlab:8000 Warning: would violate PodSecurity \"restricted:latest\": allowPrivilegeEscalation != false (container \"curl\" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container \"curl\" must set securityContext.capabilities.drop=[\"ALL\"]), runAsNonRoot != true (pod or container \"curl\" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container \"curl\" must set securityContext.seccompProfile.type to \"RuntimeDefault\" or \"Localhost\") {\"message\":\"Hello from InstructLab! Visit us at https://instructlab.ai\"}pod \"curl\" deleted NOTA kubectl run curl --image=quay.io/skupper/lanyard -n ilab-chat --restart=Never --rm -i --tty --: Este comando cria um pod temporário usando a imagem quay.io/skupper/lanyard, que é uma imagem leve para executar comandos de rede. O pod é criado no namespace ilab-chat e será removido automaticamente após a execução. --restart=Never: Garante que o pod não será reiniciado após a execução, tornando-o um pod de uso único. curl -v http://instructlab:8000/: Este comando dentro do pod executa uma requisição HTTP para o serviço do InstructLab, que está rodando na porta 8000. O -v ativa o modo verbose, mostrando detalhes da requisição e resposta. Conclusão Neste segundo ato da série, estabelecemos com sucesso a base fundamental para uma arquitetura segura de IA usando Skupper. Implementamos: O que foi Realizado Configuração do Ambiente InstructLab: Preparamos o ambiente completo para servir o modelo DeepSeek, incluindo a conversão para formato GGUF Infraestrutura Skupper Híbrida: Estabelecemos uma rede segura conectando um site público (Kubernetes) com um ambiente privado (Podman) Conexão Segura com Links: Utilizamos os novos comandos Skupper v2 (skupper link generate e skupper system setup) para criar uma conexão criptografada Arquitetura Connector/Listener: Implementamos o padrão onde o connector “empurra” o serviço privado para a rede Skupper e o listener “puxa” esse serviço para o site público Teste de Conectividade: Validamos que o modelo de IA no ambiente privado está acessível através da rede Skupper Benefícios da Arquitetura Segurança por Design: O modelo de IA permanece em ambiente controlado e privado Zero Trust Network: Skupper cria túneis seguros sem exposição direta à internet Escalabilidade: Preparação para implantação de interfaces web no cluster público Observabilidade: Base estabelecida para monitoramento de tráfego entre sites Próximos Passos No terceiro e último ato, vamos: Implantar a interface web do chatbot no Kubernetes Configurar NGINX Ingress e LoadBalancer para acesso externo Integrar a solução completa com o InstructLab Finalizar a solução end-to-end Esta arquitetura fornece a base sólida para organizações que precisam manter seus modelos de IA seguros enquanto oferecem interfaces acessíveis aos usuários finais. Referências Comandos extraídos do projeto InstructLab Primeiros passos com o InstructLab: Ajuste de modelo de IA generativo Guia de instalação do InstructLab Chatbot ILAB Frontend Hugging Face Token llama.cpp Skupper Documentation DeepSeek Model Podman Documentation" }, { "title": "Controlando e protegendo modelos de IA com segurança usando Deepseek, Skupper e InstructLab - Primeiro Ato", "url": "/posts/controlando-progetendo-ia-deepseek-skupper-istio-primeiro-ato/", "categories": "AI, Internet", "tags": "ai, deepseek, skupper, instructlab, kubernetes, security, hybrid-cloud, architecture, llm", "date": "2025-05-02 00:00:00 -0300", "content": "A história por trás deste padrão de solução A crescente demanda por aplicações orientadas por IA traz um desafio importante: como implantar e operar modelos de IA com segurança em ambientes que exigem proteção rigorosa dos dados, ao mesmo tempo em que esses modelos precisam ser acessíveis por serviços públicos. Essa necessidade ficou clara durante o desenvolvimento de um chatbot de IA local, projetado para lidar com informações sensíveis e proprietárias, o que exigiu uma solução capaz de manter o modelo protegido dentro de um ambiente seguro. Aproveitando o Skupper, a equipe criou um padrão em que os modelos de IA, servidos via InstructLab, permanecem isolados na infraestrutura privada, mas podem ser acessados de forma segura por meio de conexões protegidas em ambientes OpenShift públicos. O objetivo era garantir controle total sobre a segurança dos dados e do modelo, sem abrir mão da flexibilidade para escalar e atender às demandas dos usuários externos. Esse padrão de solução surgiu justamente da busca por equilíbrio entre segurança, desempenho e acessibilidade — especialmente para organizações que desejam adotar estratégias de nuvem híbrida. Ao utilizar o Skupper para integrar a solução de IA, a equipe viabilizou uma comunicação contínua entre ambientes privados e públicos, sem comprometer a proteção dos dados nem a eficiência operacional. Com essa arquitetura, as empresas podem continuar inovando em IA e aprendizado de máquina, mantendo a conformidade e a segurança exigidas por setores como saúde, finanças e qualquer área em que a proteção de dados seja fundamental. Um ponto crucial para essa solução também será escolher um modelo de IA rápido e eficiente, como o deepseek, que é um modelo de IA de código aberto otimizado para desempenho e segurança. O deepseek é uma excelente escolha para aplicações que exigem respostas rápidas e precisas, mantendo a privacidade dos dados. O modelo que vamos usar é o DeepSeek-R1-Distill-Qwen-1.5B , que é um modelo de IA de código aberto otimizado para desempenho e segurança, ideal para aplicações que exigem respostas rápidas e precisas. A Solução Resumo da Solução Este padrão de solução mostra, na prática, como implantar e disponibilizar com segurança um chatbot de IA local usando Skupper e InstructLab. Com essa arquitetura, é possível treinar e servir modelos de IA em um ambiente protegido, garantindo que dados sensíveis fiquem sempre seguros — mesmo quando o serviço de chatbot precisa ser acessado por usuários externos via OpenShift. Os principais componentes dessa solução são: InstructLab, responsável por gerenciar e servir os modelos de IA em uma infraestrutura privada e segura. Skupper, que estabelece uma comunicação segura e contínua entre ambientes isolados e públicos. Uma Rede Virtual de Aplicações (VAN), conectando de forma protegida dois ambientes: um site privado, onde o modelo de IA fica hospedado, e um site OpenShift público, que expõe o serviço de chatbot para os usuários externos. Um modelo de IA otimizado, como o DeepSeek-R1-Distill-Qwen-1.5B, que oferece respostas rápidas e precisas, mantendo a privacidade dos dados. Arquitetura Esta arquitetura propõe uma forma segura de implantar um chatbot de IA local, usando InstructLab e Skupper para garantir a privacidade dos dados e a comunicação contínua entre ambientes isolados. O modelo roda em um ambiente privado, enquanto o serviço é disponibilizado ao público por meio de uma implantação no OpenShift — assim, é possível oferecer acesso externo sem abrir mão da proteção dos dados e do próprio modelo. Os principais pontos dessa arquitetura são: InstructLab: responsável por hospedar, treinar e servir o modelo de IA em um ambiente seguro. DeepSeek-R1-Distill-Qwen-1.5B: um modelo de IA otimizado para desempenho e segurança, ideal para aplicações que exigem respostas rápidas e precisas. Skupper: cria uma Rede Virtual de Aplicações (VAN) entre ambientes privados e públicos, permitindo uma comunicação segura. OpenShift/Kubernetes: usado para implantar o serviço de chatbot, garantindo que ele possa ser acessado por usuários externos de forma controlada. Podman: utilizado para gerenciar contêineres de forma leve e segura, facilitando a implantação do InstructLab e do chatbot. IMPORTANTE: Essa abordagem permite que serviços hospedados em diferentes ambientes conversem de forma segura usando o Skupper, protegendo tanto os dados quanto o modelo de IA e garantindo que o acesso externo seja feito de maneira controlada e segura. Diagrama da Arquitetura Desafios Comuns Hospedagem Segura de Modelos: Proteger modelos de IA sensíveis, permitindo ao mesmo tempo acesso externo controlado. Conectividade Híbrida: Facilitar a comunicação entre ambientes privados e públicos sem comprometer a segurança. Privacidade de Dados: Garantir que os dados privados permaneçam em um ambiente protegido, ao mesmo tempo em que fornece respostas de IA em tempo real para usuários externos. Escalabilidade: Permitir que o serviço de chatbot escale conforme necessário, sem comprometer a segurança ou a privacidade dos dados. Visualização e Monitoramento: Fornecer visibilidade sobre o tráfego e as interações entre os ambientes, garantindo que a comunicação seja segura e eficiente. Conjunto de Tecnologias Plataformas e Ferramentas Utilizadas: Red Hat OpenShift O Red Hat OpenShift é uma plataforma de contêineres baseada em Kubernetes que permite aos desenvolvedores construir, implantar e gerenciar aplicações em contêineres. Ele fornece uma plataforma robusta para escalar e automatizar aplicações em ambientes de nuvem híbrida, garantindo confiabilidade e segurança. InstructLab InstructLab é uma plataforma de modelo de IA que simplifica o processo de treinamento, serviço e gerenciamento de grandes modelos de linguagem. Ele foi projetado para ser flexível e seguro, tornando-o ideal para ambientes onde os modelos precisam permanecer privados, mas ainda atender a solicitações externas por meio de caminhos de acesso controlados. DeepSeek-R1-Distill-Qwen-1.5B DeepSeek-R1-Distill-Qwen-1.5B é um modelo de IA de código aberto otimizado para desempenho e segurança, ideal para aplicações que exigem respostas rápidas e precisas. Ele é projetado para ser leve e eficiente, mantendo a privacidade dos dados enquanto oferece resultados de alta qualidade. Podman Podman é um mecanismo de contêineres que permite aos usuários gerenciar contêineres sem a necessidade de um daemon. Ele fornece um ambiente seguro e leve para executar contêineres, tornando-o ideal para implantar modelos e serviços de IA em ambientes isolados. Skupper Skupper é uma plataforma de mensagens distribuídas e seguras que permite a comunicação entre serviços em diferentes ambientes. Ele cria uma sobreposição de rede segura que permite que os serviços interajam sem expor dados sensíveis diretamente, garantindo a privacidade dos dados e a comunicação segura. Usaremos a versão cli do Skupper para criar uma conexão segura entre os ambientes privado e público. NGINX Ingress Controller NGINX Ingress Controller é uma solução para expor serviços HTTP e HTTPS de dentro de um cluster Kubernetes. Ele fornece balanceamento de carga, terminação SSL e roteamento baseado em nome, tornando-o ideal para expor aplicações web de forma segura e eficiente. Uma visão detalhada da arquitetura da solução Esta arquitetura é baseada em uma configuração híbrida onde o modelo de IA é treinado e servido via InstructLab em um ambiente privado (Ambiente Local Privado) e só pode ser acessado por usuários externos através de um site Kubernetes público, usando Skupper para comunicação segura. O Skupper garante que os dados trocados entre esses dois ambientes permaneçam seguros, criando uma Rede Virtual de Aplicações (VAN) entre os sites. Exposição de Serviços com NGINX Ingress Para expor o chatbot de IA para usuários externos, utilizamos o NGINX Ingress Controller, que oferece: Balanceamento de Carga: Distribuição eficiente de tráfego entre múltiplas instâncias Terminação SSL/TLS: Criptografia segura das conexões Roteamento Avançado: Roteamento baseado em path e host LoadBalancer Service: Acesso direto com IP externo do provedor de nuvem Essa abordagem fornece uma solução robusta e escalável para expor aplicações de IA de forma segura em ambientes Kubernetes. Visão Geral da Arquitetura O Ambiente Local Privado hospeda o modelo de IA usando InstructLab e é responsável por: Receber a entrada do usuário exclusivamente do Openshift Cluster (OpenShift). Enviar a entrada para o modelo LLaMA3 para processamento. Retornar a resposta do modelo para o Openshift Cluster com segurança. O Cluster Kubernetes público é responsável por: Expor o chatbot de IA para usuários externos via NGINX Ingress ou LoadBalancer. Enviar solicitações para o modelo InstructLab privado no Ambiente Local Privado. Exibir a resposta da IA do modelo hospedado no Ambiente Local Privado. Por design, o modelo em execução no ambiente privado (Ambiente Local Privado) é isolado e não pode ser acessado diretamente por clientes externos. Todas as interações com o modelo são mediadas pelo cluster Kubernetes público, garantindo que o modelo permaneça protegido, enquanto ainda permite que os usuários externos interajam com o serviço de chatbot de forma segura. E o modelo de IA? O modelo de IA utilizado nesta solução é o DeepSeek-R1-Distill-Qwen-1.5B, que é um modelo de IA de código aberto otimizado para desempenho e segurança, ideal para aplicações que exigem respostas rápidas e precisas. Ele é projetado para ser leve e eficiente, mantendo a privacidade dos dados enquanto oferece resultados de alta qualidade. A vantagem de usar o instructlab é que ele permite treinar e servir modelos de IA de forma segura, garantindo que os dados sensíveis permaneçam protegidos. O InstructLab facilita o processo de treinamento e serviço de modelos de IA, tornando-o uma escolha ideal para ambientes onde a segurança e a privacidade dos dados são fundamentais. Nessa solução vamos baixar o modelo DeepSeek-R1-Distill-Qwen-1.5B do Hugging Face, converter para o formato necessário e treiná-lo usando o InstructLab. A Ascensão do DeepSeek: Uma Tempestade Perfeita de Desempenho, Preço e Código Aberto O entusiasmo em torno dos modelos de IA da DeepSeek origina-se de uma poderosa combinação de desempenho impressionante, custo-benefício disruptivo e um compromisso com princípios de código aberto. Isso permitiu que a empresa conquistasse um espaço significativo em um mercado dominado por gigantes da tecnologia como OpenAI, Google e Meta. Diferenciais Chave: O Que Distingue o DeepSeek As principais distinções entre o DeepSeek e outros modelos proeminentes como GPT, Llama e Mistral residem em sua arquitetura, relação desempenho-custo e sua filosofia de código aberto. Eficiência Arquitetural: A Vantagem do Mixture of Experts (MoE) No cerne da eficiência do DeepSeek está o uso de uma arquitetura Mixture of Experts (MoE). Diferente dos modelos tradicionais que ativam todos os seus parâmetros para cada tarefa, um modelo MoE é composto por inúmeras redes “especialistas” menores. Para qualquer entrada, o modelo direciona inteligentemente a tarefa para os especialistas mais relevantes. Essa ativação esparsa resulta em custos computacionais significativamente mais baixos e tempos de inferência mais rápidos, sem uma queda proporcional no desempenho. Economia Disruptiva: Alto Desempenho a um Custo Menor Um grande catalisador para o entusiasmo em torno do DeepSeek tem sido seu notável custo-benefício. A API para usar os modelos do DeepSeek é consideravelmente mais barata que a de concorrentes como a série GPT da OpenAI. Além disso, relatos indicaram que os modelos do DeepSeek foram treinados com uma fração do orçamento de outros modelos de grande escala. Essa combinação de alto desempenho, particularmente em áreas complexas como programação e matemática, a um preço significativamente mais baixo, tornou-o uma opção atrativa para desenvolvedores e empresas. O Poder do Código Aberto Em um cenário onde muitos dos modelos mais poderosos são proprietários e de código fechado, a DeepSeek adotou uma abordagem de código aberto para muitos de seus modelos. Esta estratégia tem várias vantagens chave: Transparência e Confiança: Abrir o código do modelo permite que a comunidade global de pesquisa e desenvolvimento analise, entenda e construa sobre a tecnologia, fomentando um maior grau de confiança. Inovação Acelerada: Ao tornar seus modelos acessíveis, a DeepSeek incentiva um ambiente colaborativo onde desenvolvedores podem contribuir para sua melhoria e adaptá-lo para uma vasta gama de aplicações. Acessibilidade: Reduz a barreira de entrada para empresas menores e desenvolvedores individuais que talvez não tenham os recursos para acessar modelos proprietários caros. A Explicação do Hype: Uma Confluência de Fatores O entusiasmo em torno do DeepSeek pode ser atribuído à convergência dos fatores mencionados acima: Desafiando a Narrativa “Maior é Melhor”: O sucesso do DeepSeek demonstrou que um desempenho impressionante pode ser alcançado através de design de arquitetura e métodos de treinamento mais inteligentes, em vez de simplesmente usar quantidades massivas de poder computacional. Isso gerou uma conversa mais ampla sobre o futuro do desenvolvimento de IA e a sustentabilidade de modelos cada vez maiores. Forte Desempenho em Benchmarks: Os modelos do DeepSeek têm consistentemente se posicionado no topo ou perto do topo em vários benchmarks, especialmente nos campos exigentes de programação e matemática. Isso atraiu atenção significativa da comunidade técnica. Uma Alternativa Viável: A combinação de baixo custo e alto desempenho posicionou o DeepSeek como uma alternativa convincente aos modelos mais estabelecidos e, muitas vezes, mais caros. Isso é particularmente verdadeiro para desenvolvedores que procuram modelos poderosos e eficientes para tarefas especializadas. Dinâmicas Geopolíticas e de Mercado: Como um proeminente laboratório de IA da China, a ascensão do DeepSeek também introduziu uma nova dinâmica no cenário global de IA, desafiando o domínio de empresas sediadas nos EUA e provocando discussões sobre a democratização da tecnologia de IA avançada. Sobre o Conjunto de Tecnologias Esta solução usa o Skupper e o InstructLab para proteger a implantação do modelo de IA. O Kubernetes garante o dimensionamento flexível do serviço de chatbot com NGINX Ingress e LoadBalancer services, enquanto o Skupper permite uma comunicação contínua e segura entre os sites isolados, criando um ambiente de nuvem híbrida robusto para aplicações orientadas por IA. Próximos Passos Para ver essa solução em ação e aprender como implementá-la passo a passo, continue lendo o próximo artigo: “Controlando e protegendo modelos de IA com segurança usando Deepseek, Skupper e InstructLab - Segundo Ato”, onde abordaremos todos os detalhes técnicos da implementação. Referências Comandos extraídos do projeto InstructLab Guia de instalação do InstructLab Chatbot ILAB Frontend DeepSeek-R1-Distill-Qwen-1.5B llama.cpp Skupper Documentation Kubernetes Documentation NGINX Ingress Controller Podman Documentation" }, { "title": "Real-Time Linux: Uma Jornada de Baixa Latência", "url": "/posts/RTL-Linux/", "categories": "linux, rtos, tecnologia", "tags": "linux, real-time, rtos, kernel, preempt-rt, low-latency, sistemas-embarcados, redhat", "date": "2024-12-20 00:00:00 -0300", "content": "Introdução Real-Time Linux (RTL) é uma extensão do kernel do Linux que transforma o sistema operacional em um ambiente de tempo real. Isso significa que, além de suas funcionalidades tradicionais, ele agora é capaz de lidar com tarefas que exigem alta previsibilidade e baixa latência, como sistemas de automação industrial, dispositivos médicos e até sistemas de entretenimento. No final de 2024, o Linux Kernel passou a incorporar totalmente o Real-Time Linux (RTL), marcando um momento histórico na evolução do sistema. Mas como chegamos até aqui? A Jornada do RTL A ideia de adicionar capacidades de tempo real ao Linux remonta ao final dos anos 90, quando surgiram os primeiros patches para otimizar o desempenho e reduzir a latência do kernel. Esses patches evoluíram para projetos mais robustos, como o PREEMPT-RT. O PREEMPT-RT permitiu: Preempção total: Tornar possível a interrupção de quase todas as rotinas do kernel. Redução de latências: Melhorar a previsibilidade em execuções críticas. Depois de anos de desenvolvimento comunitário e apoio de empresas como Red Hat, Intel e IBM, o PREEMPT-RT finalmente foi fundido no kernel principal, consolidando o Linux como uma plataforma RTOS (Sistema Operacional de Tempo Real). Comparação: Workload RTL vs. Workload Normal Para cargas de trabalho típicas com requisitos de latência do kernel na faixa de milissegundos (ms), o kernel padrão do Red Hat Enterprise Linux 7 é suficiente. No entanto, se sua carga de trabalho exige requisitos rigorosos de determinismo de baixa latência para recursos centrais do kernel, como manipulação de interrupções e escalonamento de processos na faixa de microssegundos (μs), o kernel de tempo real é a escolha ideal. Referência Modelos de Preempção no Linux Os modelos de preempção do Linux determinam como o kernel gerencia interrupções e tarefas. Esses modelos são definidos no momento da compilação do kernel, sendo o “Kernel Totalmente Preemptível” essencial para obter o comportamento em tempo real. Abaixo está uma descrição dos principais modelos: Sem Preempção Forçada (Servidor): Modelo tradicional focado em maximizar a taxa de transferência. Os pontos de preempção ocorrem apenas em retornos de chamadas de sistema e interrupções. Preempção Voluntária (Desktop): Reduz a latência do kernel adicionando pontos de preempção explícitos no código, em troca de uma leve queda na taxa de transferência. Kernel Preemptível (Desktop de Baixa Latência): Faz com que todo o código do kernel, exceto seções críticas, seja preemptível, com pontos de preempção implícitos após cada desativação de preempção. Kernel Preemptível (RT Básico): Similar ao modelo “Desktop de Baixa Latência”, mas com manipuladores de interrupções em threads. Esse modelo é usado para testes e depuração. Kernel Totalmente Preemptível (RT): Todo o código do kernel é preemptível, exceto em seções críticas selecionadas. Inclui manipuladores de interrupções em threads e mecanismos como spinlocks dormêntes e rt_mutex para minimizar seções não preemptíveis, garantindo comportamento em tempo real. Onde o Real-Time Linux pode ser Aplicado? As aplicações são diversas, mas geralmente se concentram em cenários que exigem desempenho crítico: Automotivo: Sistemas de freios e controle de motores. Industrial: Robótica e automação. Telecomunicações: Redes 5G que requerem baixa latência para processamento de pacotes. Entretenimento: Mixagem de áudio em tempo real. Saúde: Equipamentos médicos sensíveis ao tempo. Exemplo Prático: Testando o Kernel PREEMPT-RT Entendendo o PREEMPT-RT O PREEMPT-RT é um conjunto de patches aplicados ao kernel Linux que permite transformar o sistema operacional em um ambiente de tempo real. Este modelo de preempção reduz drasticamente a latência ao substituir os mecanismos de sincronização convencionais por variantes que suportam preempção. Ele também implementa mecanismos para dividir seções críticas longas e forçar o encadeamento de manipuladores de interrupção. Recursos principais do PREEMPT-RT: Threading de Interrupções: Todas as interrupções são executadas como threads agendáveis, permitindo que o sistema priorize e gerencie tarefas em tempo real. Spinlocks Preemptíveis: Substitui spinlocks padrão por variantes que permitem preempção, minimizando atrasos durante o bloqueio de recursos compartilhados. Herança de Prioridade: Implementação de mutexes que evitam problemas de inversão de prioridade, garantindo que tarefas críticas recebam os recursos necessários no momento certo. Fragmentação de Seções Não Preemptíveis: Reduz o tempo de bloqueio em código crítico, quebrando longas seções não preemptíveis em pedaços menores. Esses recursos tornam o kernel Linux altamente responsivo e adequado para sistemas que exigem previsibilidade e baixa latência, como aplicações em robótica, telecomunicações e equipamentos médicos. A seguir, apresentamos um exemplo de como configurar e testar um kernel com suporte a PREEMPT-RT. Essas instruções foram realizadas em uma máquina virtual Ubuntu 22.04 LTS. Passos: 1. Obter o código do kernel mais recente: git clone git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git cd linux 2. Configurar o kernel para PREEMPT-RT: make menuconfig Navegue até General Setup / Preemption Model e ative a opção Fully Preemptible Kernel (Real-Time). Salve e saia. Verifique se a configuração foi aplicada: grep PREEMPT_RT .config # Saída esperada: CONFIG_PREEMPT_RT=y 3. Compilar o kernel: time make -j$(nproc) 4. Instalar e reiniciar: sudo make modules_install &amp;&amp; sudo make install sudo reboot Selecione o novo kernel na inicialização. 5. Confirmar a versão: cat /proc/version # Exemplo de saída: # Linux version 6.11.0-rtl+ (gcc (Ubuntu 11.4.0) 11.4.0) #1 SMP PREEMPT_RT Fri Sep 20 19:11:35 IST 2024 Agora o kernel está configurado para suportar tarefas de tempo real. Para verificar sua eficiência, execute aplicativos em tempo real como mixagem de áudio com JACK ou PulseAudio. Como Configurar o Red Hat Enterprise Linux for Real Time O Red Hat Enterprise Linux for Real Time (RHEL-RT) oferece um conjunto de ferramentas e configurações otimizadas para aplicações sensíveis à latência. Seguem os passos para configurar e aproveitar ao máximo o RHEL-RT: 1. Pré-requisitos Certifique-se de que sua subscrição Red Hat inclui o canal do RHEL for Real Time. Atualize seu sistema: sudo dnf update -y 2. Instalação do Kernel RT Instale o kernel otimizado para tempo real: sudo dnf install kernel-rt kernel-rt-devel 3. Seleção do Kernel RT no Bootloader Depois de instalar o kernel, configure o bootloader para usar o kernel RT como padrão: grub2-set-default \"Red Hat Enterprise Linux (kernel-rt)\" sudo grub2-mkconfig -o /boot/grub2/grub.cfg sudo reboot 4. Ajustes de Latência Após reiniciar no kernel RT, use ferramentas como tuna para ajustar prioridades e afinidades de CPU: sudo tuna -t irq -q sudo tuna -t \"[sua aplicação]\" -p 99 Configure o particionamento da CPU para isolar núcleos dedicados a tarefas de tempo real: echo 1 &gt; /sys/devices/system/cpu/cpu[X]/isolated 5. Ferramentas de Diagnóstico Utilize ferramentas como cyclictest para medir a latência: sudo cyclictest -m -Sp99 -i100 -h300 -q Como o RTL foi Implementado pela Red Hat? A Red Hat contribuiu de forma significativa para a integração do PREEMPT-RT no kernel principal. No Red Hat Enterprise Linux for Real Time, várias otimizações foram feitas para: Otimizar latências: Ajustes no agendador e gerenciamento de interrupções. Ferramentas especializadas: Perfis de kernel personalizados e ferramentas como tuna para ajustar prioridades em tempo real. Segurança: Garantir que as modificações não comprometem a estabilidade do sistema. Conclusão A inclusão do Real-Time Linux no kernel principal marca um ponto de virada para o Linux como um sistema operacional universal. Agora, ele é capaz de atender tanto aplicações convencionais quanto ambientes que exigem altíssima previsibilidade. Seja você um desenvolvedor interessado em explorar o potencial do Linux em sistemas embarcados ou um entusiasta de tecnologia, o RTL abre novas portas para inovações em diversas áreas. Referências Linux Foundation. “Real-Time Linux Documentation.” Disponível em: https://wiki.linuxfoundation.org/realtime/documentation/start. Acesso em: 20 dez. 2024. Linux Foundation. “Kernel PREEMPT-RT.” Disponível em: https://wiki.linuxfoundation.org/realtime/documentation/technical_basics/preemption_models. Acesso em: 20 dez. 2024. Red Hat. “Red Hat Enterprise Linux for Real Time.” Disponível em: https://docs.redhat.com/en/documentation/red_hat_enterprise_linux_for_real_time/7/html/installation_guide/chap-why_use_rt_to_optimize_latency#chap-Why_Use_RT_to_Optimize_Latency. Acesso em: 20 dez. 2024. Kaiwan N Billimoria. “The Linux Kernel is Now an RTOS.” Blog Kaiwan Tech. Disponível em: https://kaiwantech.wordpress.com/2024/09/21/the-linux-kernel-is-now-an-rtos-with-rtl-being-fully-merged/. Acesso em: 20 dez. 2024." }, { "title": "TCP/IP O início e a evolução", "url": "/posts/protocolos-TCP-IP-o-inicio/", "categories": "networking, tcpip", "tags": "networking, tcp-ip, protocolos, arpanet, internet, historia, tecnologia, redes", "date": "2024-12-14 00:00:00 -0300", "content": "Protocolos TCP/IP: Uma Introdução Completa O ARPANET e as Origens do TCP/IP A história dos protocolos TCP/IP começa no final dos anos 1950, durante o auge da Guerra Fria. O Departamento de Defesa dos EUA (DoD) buscava uma rede de comando e controle que pudesse sobreviver a um ataque nuclear. Na época, as comunicações militares utilizavam a rede telefônica pública, considerada vulnerável devido à sua hierarquia rígida e falta de redundância. Início da Pesquisa e Contribuições de Paul Baran Em torno de 1960, o DoD contratou a RAND Corporation para encontrar uma solução. Paul Baran propôs um design distribuído altamente tolerante a falhas, que usava tecnologia de comutação de pacotes digitais em vez de sinais analógicos. Apesar da resistência inicial de grandes empresas como a AT&amp;T, a ideia de Baran lançou as bases para as redes resilientes modernas. O Papel da ARPA Em resposta ao lançamento do satélite Sputnik pela União Soviética em 1957, o governo dos EUA criou a ARPA (Advanced Research Projects Agency). A ARPA iniciou esforços em redes de computadores para promover a pesquisa científica e tecnológica. Larry Roberts, um dos gerentes da ARPA, decidiu construir uma rede baseada em comutação de pacotes, influenciado pelo trabalho de Paul Baran e Donald Davies. Estrutura do ARPANET A ARPANET foi projetada como uma rede de sub-redes de comutação de pacotes composta por minicomputadores chamados IMPs (Interface Message Processors). Esses IMPs, conectados por linhas de transmissão de 56 kbps, formaram a primeira rede eletrônica de comutação de pacotes que utilizava o método de armazenamento e encaminhamento para transmitir dados de forma confiável. Cada nó da rede era composto por um IMP e um host, conectados localmente por fios curtos. Mensagens de até 8063 bits eram divididas em pacotes menores e transmitidas de forma independente, permitindo o roteamento dinâmico caso partes da rede fossem destruídas. A rede foi projetada para ser resiliente, com cada IMP conectado a pelo menos dois outros, garantindo redundância. Crescimento e Impacto A ARPANET entrou em operação em dezembro de 1969 com quatro nós iniciais: UCLA, UCSB, SRI e a Universidade de Utah. Em poucos anos, a rede cresceu rapidamente, conectando mais instituições e redes. Durante os anos 1980, a integração de novas redes e a criação do DNS (Domain Name System) facilitaram o gerenciamento de endereços e nomes de host. Essa expansão culminou na criação dos protocolos TCP/IP, projetados para interconectar redes heterogêneas. Contratos foram estabelecidos com universidades e empresas para implementar os protocolos em diferentes plataformas, consolidando o TCP/IP como padrão global de comunicação em redes. Introdução ao Modelo TCP/IP Os protocolos TCP/IP (Transmission Control Protocol/Internet Protocol) formam a espinha dorsal da comunicação na internet moderna. Este artigo fornece uma revisão abrangente sobre os protocolos, suas camadas e como eles trabalham juntos para permitir a transmissão de dados em redes locais e globais. O Modelo TCP/IP: Introdução e Primeira Camada O modelo TCP/IP é um framework fundamental para a comunicação de dados em redes modernas, estruturado em quatro camadas principais. Cada camada desempenha uma função específica, permitindo que os dados sejam transmitidos de forma eficiente e confiável. Neste artigo, começaremos com uma análise detalhada da primeira camada: Camada de Aplicação. Camada de Aplicação: A Interface do Usuário A Camada de Aplicação no modelo TCP/IP representa o ponto de contato direto entre os usuários e a rede. Ela oferece as ferramentas e protocolos necessários para que aplicativos e serviços possam interagir com o sistema de comunicação. Esta camada traduz solicitações e respostas de aplicações em dados compreensíveis pelas camadas inferiores do modelo. Essa camada também é conhecida como a Camada 7 no modelo OSI (Open Systems Interconnection), que é um modelo de referência semelhante ao TCP/IP. No entanto, o modelo TCP/IP é mais amplamente utilizado e mais diretamente relacionado à arquitetura da internet. Funcionalidades Principais Interação com Aplicações: Oferece serviços que permitem que aplicativos de software utilizem a rede para transmitir dados. Processamento de Dados: Manipula a estrutura dos dados para garantir compatibilidade com protocolos de transporte. Serviços de Rede: Facilita a implementação de serviços específicos, como transferência de arquivos, envio de e-mails e navegação na web. Exemplos de Protocolos na Camada de Aplicação HTTP/HTTPS (Hypertext Transfer Protocol): Facilita a comunicação entre navegadores e servidores web, essencial para a navegação na internet. FTP (File Transfer Protocol): Utilizado para a transferência de arquivos entre computadores. SMTP (Simple Mail Transfer Protocol): Gerencia o envio de e-mails. DNS (Domain Name System): Resolve nomes de domínio em endereços IP. Estrutura Técnica Os protocolos na Camada de Aplicação não apenas facilitam a comunicação, mas também incorporam funcionalidades como autenticação, compressão e criptografia. Por exemplo, o HTTPS adiciona segurança ao HTTP usando o protocolo SSL/TLS para criptografar dados transmitidos. Modelo de Dados Os dados processados nesta camada são encapsulados e formatados em mensagens, que serão transmitidas para a próxima camada. A seguir, está o fluxo de dados típico dentro da camada de aplicação: Aplicações no Dia a Dia A Camada de Aplicação é amplamente utilizada por programas que fazem parte do nosso cotidiano: Navegadores Web: Ao acessar um site, como “www.example.com”, o navegador utiliza o HTTP ou HTTPS para enviar solicitações e receber respostas do servidor web. Clientes de E-mail: Programas como Microsoft Outlook ou Thunderbird utilizam protocolos como SMTP, IMAP ou POP3 para enviar e receber mensagens. Streaming de Vídeo: Serviços como Netflix e YouTube empregam protocolos como HTTP/HTTPS para entrega de vídeos, muitas vezes utilizando redes de distribuição de conteúdo (CDN). Aplicativos de Mensagens: WhatsApp e Telegram utilizam protocolos de comunicação baseados em HTTP/HTTPS e outros serviços da camada de aplicação para troca de mensagens instantâneas e arquivos. Jogos Online: Muitos jogos dependem de APIs baseadas em HTTP/HTTPS para autenticação e sincronização de dados, além de outros protocolos específicos. Importância na Arquitetura de Redes A Camada de Aplicação é considerada crítica porque estabelece os fundamentos para a interação humano-computador nas redes. Sem esta camada, o uso prático da internet seria impossível, pois não haveria um meio eficaz de traduzir as intenções humanas em solicitações processáveis pela rede. Testando a Camada de Aplicação com Python Para demonstrar a funcionalidade da Camada de Aplicação, podemos realizar um teste prático utilizando um script em Python que simula uma solicitação HTTP para um servidor web. Aqui está o código: # Importar a biblioteca de sockets para comunicação de rede import socket # Função para testar a camada de aplicação usando o protocolo HTTP def test_application_layer(host: str, port: int): \"\"\"Função para testar a camada de aplicação usando o protocolo HTTP.\"\"\" try: # Criar um socket para comunicação com o servidor with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as client_socket: # Conectar ao servidor especificado pelo host e porta client_socket.connect((host, port)) # Preparar uma solicitação HTTP GET simples http_request = f\"GET / HTTP/1.1\\r\\nHost: {host}\\r\\nConnection: close\\r\\n\\r\\n\" # Enviar a solicitação HTTP para o servidor client_socket.sendall(http_request.encode()) # Receber a resposta do servidor em partes (chunks) response = b\"\" while True: # Receber um chunk de dados do servidor chunk = client_socket.recv(4096) # Se não houver mais dados, interromper o loop if not chunk: # Se não houver mais dados, interromper o loop break # Adicionar o chunk recebido à resposta completa response += chunk # Exibir a resposta completa do servidor no console print(\"Resposta do Servidor:\") # Decodificar a resposta binária em texto print(response.decode()) except Exception as e: # Capturar e exibir qualquer erro que ocorra durante o teste print(f\"Erro ao testar a camada de aplicação: {e}\") # Testar a função com um servidor HTTP específico test_application_layer(\"www.example.com\", 80) Output Esperado Quando executado, o script realiza uma solicitação HTTP para www.example.com e exibe a resposta do servidor. O output será semelhante ao seguinte: $ python aplicacao.py Resposta do Servidor: HTTP/1.1 200 OK Age: 119377 Cache-Control: max-age=604800 Content-Type: text/html; charset=UTF-8 Date: Sat, 14 Dec 2024 03:53:09 GMT Etag: \"3147526947+ident\" Expires: Sat, 21 Dec 2024 03:53:09 GMT Last-Modified: Thu, 17 Oct 2019 07:18:26 GMT Server: ECAcc (mid/8790) Vary: Accept-Encoding X-Cache: HIT Content-Length: 1256 Connection: close &lt;!doctype html&gt; &lt;html&gt; . . . Essa demonstração prática destaca como os protocolos na Camada de Aplicação, como HTTP, facilitam a comunicação entre clientes e servidores em redes modernas. Sem esta camada, o uso prático da internet seria impossível, pois não haveria um meio eficaz de traduzir as intenções humanas em solicitações processáveis pela rede. Referências Tanenbaum, A. S., &amp; Wetherall, D. J. (2011). Computer Networks. Baran, P. (1964). On Distributed Communications: Introduction to Distributed Communications Network. Roberts, L. (1967). Multiple Computer Networks and Intercomputer Communication. Braden, R. (1989). RFC 1122: Requirements for Internet Hosts - Communication Layers. Clark, D. D. (1988). The Design Philosophy of the DARPA Internet Protocols. Cerf, V., &amp; Kahn, R. E. (1974). A Protocol for Packet Network Intercommunication. Próximos Passos Nos próximos artigos, exploraremos as camadas subsequentes do modelo TCP/IP, detalhando suas funções e protocolos associados. A próxima será a Camada de Transporte, onde examinaremos o papel do TCP e UDP na comunicação confiável e eficiente." }, { "title": "Running local AI with Instruct Lab and Skupper", "url": "/posts/running-local-ai-with-instruct-lab/", "categories": "skupper, instructlab, chatbot", "tags": "ai, instructlab, skupper, llm, chatbot, machine-learning, openshift, local-ai, security", "date": "2024-08-01 00:00:00 -0300", "content": "Welcome to the Ollama Pilot. Problem to solve The main goal of this project is to create a secure connection between two sites, enabling the communication between the engineer machine and an Instruct Lab Model. The merlinite-7b-lab-Q4_K_M.gguf model will be used for the chatbot, and it is available in the Instruct Lab. The license of the model is available in the Instruct Labs. But, why the banner? Well, the engineer needs to know who is better, Lebron or Jordan. The chatbot will be responsible for answering this question. The chatbot will receive the user input and send it to the llama3 model. The response from the merlinite model will be sent back to the user. Disclaimer All the models used are available in the Hugging Face model hub. The models are not hosted in this project, they are hosted by Hugging Face. The models are used for educational purposes only. Why InstructLab There are many projects rapidly embracing and extending permissively licensed AI models, but they are faced with three main challenges: Contribution to LLMs is not possible directly. They show up as forks, which forces consumers to choose a “best-fit” model that isn’t easily extensible. Also, the forks are expensive for model creators to maintain. The ability to contribute ideas is limited by a lack of AI/ML expertise. One has to learn how to fork, train, and refine models to see their idea move forward. This is a high barrier to entry. There is no direct community governance or best practice around review, curation, and distribution of forked models. This snippet was extracted from the Instruct Labs repository. Why Skupper Here the answer is simple, Skupper is a tool that enables secure communication between services in different environments. Skupper will be used to create a secure connection between the two sites, one of the sites has restricted access to the internet. Skupper will enable the communication between the two sites, allowing the Ollama Pilot application to send requests to the llama3 model thru the Instruct Lab chat. Description This project has the objective to create a VAN (Virtual Application Network) that enables the connection between two sites: Site A: A server that hosts the Instruct Lab chat model. This model will be responsible for receiving the user input and sending it to the llama3 model. The response from the llama3 model will be sent back to the user. Site B: An OpenShift site that exposes the Instruct Lab chat model. This site will be responsible for sending the user input to the Instruct Lab chat model and receiving the response from the Merlinite-7b-lab-Q4_K_M.gguf model. In order to connect the two sites, we will use Skupper, a tool that enables secure communication between services in different environments. Skupper will be used to create a secure connection between the two sites, allowing the Ollama Pilot application to send requests to the llama3 model and receive the response from the merlinite model. At the end of the project, you will be able to use your own CHATBOT with protected data. Architecture Summary AI model deployment with InstructLab Private Skupper deployment Public Skupper deployment Secure communication between the two sites with Skupper Chatbot with protected data 1. AI model deployment with InstructLab The first step is to deploy the InstructLab chat model in the InstructLab site. The InstructLab chat model will be responsible for receiving the user input and sending it to the llama3 model. The response from the llama3 model will be sent back to the user. This is based on the article: Getting started with InstructLab for generative AI model tuning mkdir instructlab &amp;&amp; cd instructlab python3.11 -m venv venv source venv/bin/activate pip install 'instructlab[cuda]' -C cmake.args=\"-DLLAMA_CUDA=on\" -C cmake.args=\"-DLLAMA_NATIVE=off\" IMPORTANT: This installation method will enable your Nvidia GPU to be used by instructlab. If you don’t have an Nvidia GPU, please check other options in: InstructLab 🐶 (ilab) ilab config init To enable external access to your model, please modify the config.yaml file: chat: context: default greedy_mode: false logs_dir: data/chatlogs max_tokens: null model: models/merlinite-7b-lab-Q4_K_M.gguf session: null vi_mode: false visible_overflow: true general: log_level: INFO generate: chunk_word_count: 1000 model: models/merlinite-7b-lab-Q4_K_M.gguf num_cpus: 10 num_instructions: 100 output_dir: generated prompt_file: prompt.txt seed_file: seed_tasks.json taxonomy_base: origin/main taxonomy_path: taxonomy serve: gpu_layers: -1 host_port: 0.0.0.0:8000 # HERE max_ctx_size: 4096 model_path: models/merlinite-7b-lab-Q4_K_M.gguf Now, you can download and start your server: ilab model download ilab model serve # The output should be similar to: INFO 2024-07-30 18:59:01,199 serve.py:51: serve Using model 'models/merlinite-7b-lab-Q4_K_M.gguf' with -1 gpu-layers and 4096 max context size. INFO 2024-07-30 18:59:01,611 server.py:218: server Starting server process, press CTRL+C to shutdown server... INFO 2024-07-30 18:59:01,612 server.py:219: server After application startup complete see http://0.0.0.0:8000/docs for API. 2. Private Skupper deployment The second step is to deploy the private Skupper in Site A. The private Skupper will be responsible for creating a secure connection between the two sites, allowing the Ollama Pilot application to send requests to the llama3 model and receive the response from the merlinite model. Open a new terminal and run the following commands: Install Skupper export SKUPPER_PLATFORM=podman skupper init --ingress none Exposing the InstructLab chat model In order to do this, we will bind the local service that is running the InstructLab chat model to the Skupper service. skupper expose host host.containers.internal --address instructlab --port 8000 Let’s check the status of the Skupper service: skupper service status Services exposed through Skupper: ╰─ instructlab:8000 (tcp) Now, we are almost ready to connect the two sites. The next step is to deploy the public Skupper in Site B and create a connection between the two sites. 3. Public Skupper deployment The third step is to deploy the public Skupper in Site B. The public Skupper will receive the connection from the private Skupper and create a secure connection between the two sites. Open a new terminal and run the following commands: Creating the project and deploying the public Skupper: oc new-project ollama-pilot skupper init --enable-console --enable-flow-collector --console-user admin --console-password admin Creating the token to allow the private Skupper to connect to the public Skupper: skupper token create token.yaml At this point, you should have a token.yaml file with the token to connect the two sites. The next step is to link the two sites. For this, we will need to switch back to the terminal where the private Skupper is running. 4. Secure communication between the two sites with Skupper The fourth step is to connect the two sites. In the terminal where the private Skupper is running, run the following command: skupper link create token.yaml --name instructlab # Or any other name you want Let’s check the status of the Skupper link: skupper link status Links created from this site: Link instructlab is connected Current links from other sites that are connected: There are no connected links Before continuing, let’s hop back to the terminal where the public Skupper is running and check the status of the link: skupper link status Links created from this site: There are no links configured or connected Current links from other sites that are connected: Incoming link from site b8ad86d5-9680-4fea-9c07-ea7ee394e0bd 5. Chatbot with protected data Now the last part is to expose the service in the public Skupper and create the Ollama Pilot application. Still on the terminal where the public Skupper is running, run the following command to expose the service. With the following command, we will create a Skupper service that matches the service exposed by the private Skupper. This will end up creating a Kubernetes service that will be used by the Ollama Pilot application. skupper service create instructlab 8000 Exposing the service to the internet: oc expose service instructlab Getting the public URL: oc get route instructlab NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD instructlab instructlab-ollama-pilot.apps.your-cluster-url instructlab port8000 None The last step is to create the Ollama Pilot application. The Ollama Pilot application will be responsible for sending the user input to the Instruct Lab chat model and receiving the response from the Merlinite-7b-lab-Q4_K_M.gguf model. The Ollama Pilot application will be able to send requests to the Instruct Lab chat model through the secure connection created by Skupper. You can repeat all the instructions in step 1. AI model deployment with InstructLab to install the Instruct Lab chat model in Site B. The only difference is that you will not run the ilab model serve command because the Instruct Lab chat model is already running in Site A. The Ollama Pilot application will be responsible for sending the user input to the Instruct Lab chat model and receiving the response from the Merlinite-7b-lab-Q4_K_M.gguf model. The Ollama Pilot application will be able to send requests to the Instruct Lab chat model through the secure connection created by Skupper. ilab model chat --endpoint-url http://instructlab-ollama-pilot.apps.your-cluster-url/v1/ ╭─────────────────────────────────────────────── system ──────────────────────────────────────────────── │ Welcome to InstructLab Chat w/ MODELS/MERLINITE-7B-LAB-Q4_K_M.GGUF (type /h for help) ╰────────────────────────────────────────────────────────────────────────────────────────────────────── &gt;&gt;&gt; [S][default] THE question: Yes or No question. Don’t fool me. Is LeBron better than Jordan? Have fun with your new chatbot with protected data! If you don’t agree with the answer, you can always ask again and train your model, but King James is the best!" }, { "title": "Vim Motions: Navigating and Editing Code Efficiently", "url": "/posts/vim-motions/", "categories": "vim, productivity", "tags": "vim, neovim, astrovim, editor, produtividade, motions, desenvolvimento, tools", "date": "2024-07-02 14:31:00 -0300", "content": "Vim Motions: Your Keyboard Shortcut Powerhouse Vim motions are a set of commands that empower you to navigate and edit code like a pro. This guide dives into different Vim motions and how to harness them for lightning-fast coding. My Vim Journey: From VSCode and IntelliJ to Neovim and AstroVim After years of using VSCode and IntelliJ, I decided to make the switch to Neovim as my primary development tool. Neovim is a hyperextensible Vim-based text editor that provides a modern and more powerful experience compared to traditional Vim. Recently, I transitioned to AstroVim, a Neovim configuration that comes with a set of built-in plugins and features that enhance productivity. AstroVim offers an intuitive file explorer, search functionality, and other built-in plugins that make the development experience smoother and more efficient. The decision to switch was driven by the need for a more integrated and user-friendly environment, which AstroVim provides out of the box. This series of posts will document my journey and share the tips, tricks, and configurations that have made Neovim and AstroVim indispensable parts of my workflow. After years of using VSCode and IntelliJ, I decided to make the switch to Neovim as my primary development tool. This series of posts will document my journey and share the tips, tricks, and configurations that have made Neovim an indispensable part of my workflow. Today, we’re starting with the foundation: Vim motions. Mastering these commands is key to unlocking the full potential of Vim’s editing power. Disclaimer: This is not meant to be a flamewar about editors. It’s simply a reflection of what has worked best for me and an invitation for you to explore if Vim might be a good fit for your own development style. Vim Plugins: My Productivity Arsenal AstroVim comes with a curated set of plugins that enhance the Neovim experience. Here are some of the default plugins included in AstroVim: “nvim-treesitter” - Syntax highlighting and code navigation “telescope.nvim” - Fuzzy finder and file explorer “nvim-lspconfig” - Language Server Protocol configurations “nvim-cmp” - Autocompletion plugin “gitsigns.nvim” - Git integration “lualine.nvim” - Status line “which-key.nvim” - Keybinding helper “nvim-tree.lua” - File explorer “bufferline.nvim” - Buffer line for managing open files “plenary.nvim” - Lua functions used by many plugins “copilot.vim” - AI-powered code completion “catppuccin/nvim” - Catppuccin Macchiato theme for Neovim (Complete plugin setup and details will be covered in future posts.) The Vim Editor: More Than Meets the Eye Vim is a highly customizable text editor known for its modal nature. Its modes – Normal, Insert, and Visual – cater to distinct tasks, allowing you to switch seamlessly between command input and text editing. Vim Plugins: Supercharge Your Vim Experience Vim’s capabilities extend far beyond its core features. With a vast array of community-maintained plugins, you can transform Vim into a full-fledged Integrated Development Environment (IDE). Vim Motions in Action By mastering Vim motions, you’ll write and navigate code with impressive speed. Even popular IDEs like VS Code offer Vim plugins, enabling you to leverage Vim’s navigation within your preferred environment. Vim Modes: Normal Mode: Your command center for issuing instructions. Insert Mode: Where the actual code writing happens. Visual Mode: Select and manipulate text visually, with options for both normal and block selection. Vim also has a leader key for creating custom shortcuts, and a command mode for operations like saving files. Navigating Your Codebase AstroVim enhances navigation with additional keybindings that streamline your workflow: Window Navigation (Ctrl + h/j/k/l): Quickly move between split windows using Ctrl combined with h, j, k, or l. Buffer Navigation (]b, [b): Use ]b to move to the next buffer and [b to move to the previous buffer, making it easy to switch between open files. Resize Windows (Ctrl + Arrow Keys): Adjust window sizes with Ctrl and the arrow keys for up, down, left, and right. Toggle Neotree (Leader + e): Open or close the file explorer with Leader + e for quick access to your project files. Toggle Comment (Leader + /): Comment or uncomment lines with Leader + /, streamlining code documentation and debugging. Open Terminal (Leader + tf): Launch a floating terminal with Leader + tf for quick command-line access without leaving Neovim. Let’s dive into a React app and explore how Vim motions streamline navigation: Basic Movements (h, j, k, l): Forget arrow keys! Use these for left, down, up, and right movement. Prefix with a number (e.g., 5j) to move multiple lines. Word Navigation (w, b, e): Jump forward (w), backward (b), or to the end (e) of words. Use a number prefix for multiple word jumps. Line Navigation (0, ^, g, $, f, F): Go to the start (0), first non-blank character (^), end ($), or search for a character (f forward, F backward). Vertical Navigation ((), {}, Ctrl+D/U, Ctrl+F/B, G): Navigate by sentences (( and )), paragraphs ({ and }), half pages (Ctrl+D, Ctrl+U), full pages (Ctrl+F, Ctrl+B), start of file (gg), and end of file (G). Entering Insert Mode: Multiple Entry Points Vim offers various ways to enter insert mode, each suited for different editing scenarios: Before cursor (i): Use this when you need to insert text right before the current cursor position. For example, adding a missing character in a variable name. After cursor (a): Ideal for appending text immediately after the cursor. This is useful when you want to add a semicolon at the end of a statement. Beginning of line (I): Quickly jump to the start of a line to insert text. This is handy for adding comments or annotations at the start of a line of code. End of line (A): Move to the end of a line to append text. Use this when you need to extend a line with additional code or comments. Below current line (o): Open a new line below the current one and enter insert mode. This is perfect for adding a new line of code in a block. Above current line (O): Similar to o, but opens a new line above. Use this when you need to insert a line before the current one, such as adding a new function definition. Other insert mode triggers include c (change), s (substitute), y (yank/copy), and p (paste). You can even copy entire lines with yy. Learning More Vim’s learning curve can be steep, but the rewards are immense. To deepen your knowledge: Vim Documentation: https://vimdoc.sourceforge.io/index.html Vim Tutorial: https://www.tutorialspoint.com/vim/vim_tutorial.htm Vim Cheat Sheet: https://www.vim.org/doc/vimtutor/vimtutor.pdf Happy Vimming!" }, { "title": "Workshop: Patient Portal, conectando um banco de dados a um cluster K8S com Skupper", "url": "/posts/workshop-skupper-patient-portal/", "categories": "skupper, multi, cloud, network, redhat", "tags": "skupper, workshop, kubernetes, openshift, networking, database, tutorial, hands-on", "date": "2024-07-02 00:00:00 -0300", "content": "Descrição Este workshop tem como objetivo apresentar o Red Hat Service Interconnect, uma solução de integração de aplicações que permite a comunicação entre diferentes sistemas de forma eficiente e segura. Arquitetura da Solução Topologia de Serviços Resumo do Workshop Logar no Red Hat Developer. Criar um cluster Openshift. Acessar o cluster Openshift. No projeto do Red Hat Openshift Sandbox, acessar o seu projeto. Criar uma máquina virtual no Openshift Virtualization. Instalar pacotes na máquina virtual: podman kubernetes-client skupper oc wget Fazer o deploy do banco de dados com o podman. Fazer o deploy do frontend e do backend da aplicação. Configurar o Service Interconnect (Skupper) para fazer a comunicação do banco de dados rodando em um podman com a aplicação rodando no Openshift. Acessar a aplicação e verificar se a comunicação está funcionando. Considerações. Links Recurso Link [1] Red Hat Developer https://developers.redhat.com/ [2] OC https://docs.openshift.com/container-platform/4.15/cli_reference/openshift_cli/getting-started-cli.html [3] Skupper https://skupper.io/ Pré-requisitos Conta no Red Hat Developer Conhecimento básico em Kubernetes Conhecimento básico em Red Hat OpenShift Conhecimento básico em Podman Google Chrome, a preferência por ele é pela funcionalidade de colar comandos no console da máquina virtual pelo VNC via browser. Passo a passo 1. Logar no Red Hat Developer Acesse o site do Red Hat Developer e faça o login com a sua conta. 2. Criar um cluster Openshift Sandbox 2.1. Acesse o Red Hat Openshift Sandbox e clique em “Start Cluster”. 2.2. Inicie o cluster Openshift Sandbox. 4. Acessar o cluster Openshift Acesse o cluster Openshift Sandbox e clique em “Open Console”. 5. No projeto do Red Hat Openshift Sandbox, acessar o seu projeto Clique no seu projeto e mude para a view “Administrator”. 6. Criar uma máquina virtual no Openshift Virtualization Acesse Virtualization no menu do cluster Openshift e clique em Virtual Machines. Clique em Create Virtual Machine. Escolha From Template e selecione o template Fedora VM. Clique em Create VirtualMachine. 7. Instalar pacotes na máquina virtual Acesse a máquina virtual e clique em Console. (Dê preferencia para o Google Chrome, pois ele tem a funcionalidade de colar comandos no console da máquina virtual pelo VNC via browser). Logue com as credenciais que estão no console. Execute os comandos abaixo para instalar os pacotes necessários: sudo dnf install -y podman kubernetes-client wget # Instalar o oc wget -qO- https://mirror.openshift.com/pub/openshift-v4/clients/ocp/stable/openshift-client-linux.tar.gz | tar xz -C ~/.local/bin export PATH=\"$HOME/.local/bin:$PATH\" # Instalar o skupper curl https://skupper.io/install.sh | sh 8. Fazer o deploy do banco de dados com o podman Execute o comando abaixo para fazer o deploy do banco de dados: podman network create skupper podman run --name database-target --network skupper --detach --rm -p 5432:5432 quay.io/skupper/patient-portal-database 9. Fazer o deploy do frontend e do backend da aplicação No seu console openshift, faça o deploy do seguinte yaml para o frontend: apiVersion: apps/v1 kind: Deployment metadata: labels: app: frontend name: frontend spec: replicas: 3 selector: matchLabels: app: frontend template: metadata: labels: app: frontend spec: containers: - name: frontend image: quay.io/skupper/patient-portal-frontend env: - name: DATABASE_SERVICE_HOST value: database - name: DATABASE_SERVICE_PORT value: \"5432\" - name: PAYMENT_PROCESSOR_SERVICE_HOST value: payment-processor - name: PAYMENT_PROCESSOR_SERVICE_PORT value: \"8080\" ports: - containerPort: 8080 No seu console openshift, faça o deploy do seguinte yaml para o backend: apiVersion: apps/v1 kind: Deployment metadata: labels: app: payment-processor name: payment-processor spec: replicas: 3 selector: matchLabels: app: payment-processor template: metadata: labels: app: payment-processor spec: containers: - name: payment-processor image: quay.io/skupper/patient-portal-payment-processor ports: - containerPort: 8080 10. Configurar o Service Interconnect (Skupper) para fazer a comunicação do banco de dados rodando em um podman com a aplicação rodando no Openshift. Para isso, vamos dividir em 3 etapas: Configuração do Cluster Kubernetes Configuração do Site Podman Expor o serviço do banco de dados para a VAN do Skupper Configuração do Cluster Kubernetes: Iniciar o skupper no cluster oenshift com o console habilitado skupper init --enable-console --enable-flow-collector --console-user admin --console-password admin Acessar o console do skupper, para isso acesse as rotas do seu cluster Openshift, a URL estará lá. Criando um token para conectar o site podman com o cluster Openshift skupper token create ./skupper-token.yaml Configuração do Site Podman: Acesse a máquina virtual e execute o comando abaixo para conectar o site podman com o cluster Openshift Ininie o skupper no site podman, sem ingress. skupper switch podman # para mudar o contexto para podman o padrão é kubernetes Conecte o site podman com o cluster Openshift skupper link create ./skupper-token.yaml Acesse o console do skupper no cluster Openshift e verifique se o site podman está conectado. Expor o serviço do banco de dados para a VAN do Skupper: Expor o serviço do banco de dados para a VAN do Skupper systemctl --user enable --now podman.socket skupper service create database 5432 skupper service bind database host database-target --target-port 5432 No cluster Openshift, vamos criar um serviço Skupper para o banco de dados, esse serviço vai apontar para o serviço do banco de dados que está rodando no site podman, através da VAN do Skupper. skupper service create database 5432 Agora, a aplicação frontend e backend estão se comunicando com o banco de dados que está rodando em um site podman, através da VAN do Skupper. 13. Acessar a aplicação e verificar se a comunicação está funcionando Para isso, vamos precisar executar algumas tarefas para expor o frontend no cluster Openshift. Criar um serviço para o fronend que aponte para o deployment dele use o seguinte YAML: apiVersion: v1 kind: Service metadata: name: frontend namespace: SEU-NAME-SPACE spec: selector: app: frontend ports: - protocol: TCP port: 8080 targetPort: 8080 Criar uma rota que aponte para o serviço do frontend. kind: Route apiVersion: route.openshift.io/v1 metadata: name: frontend namespace: SEU-NAME-SPACE labels: {} spec: to: kind: Service name: frontend tls: {} port: targetPort: 8080 alternateBackends: [] 14. Considerações O Red Hat Service Interconnect (Skupper): Oferece uma solução poderosa para integrar aplicações em diferentes ambientes, simplificando a comunicação entre serviços e proporcionando maior flexibilidade e escalabilidade. Ao abstrair a complexidade da rede subjacente, o Skupper permite que os desenvolvedores se concentrem na lógica de negócios de suas aplicações, sem se preocupar com os detalhes de conectividade. Com recursos como descoberta de serviços automática: O roteamento inteligente e segurança integrada, o Skupper garante que as aplicações possam se comunicar de forma eficiente e segura, independentemente de sua localização. Essa abordagem simplifica a gestão da infraestrutura e reduz a necessidade de configurações manuais, agilizando o desenvolvimento e a implantação de aplicações distribuídas. Além disso, o Skupper oferece uma interface de usuário intuitiva e ferramentas de linha de comando poderosas, facilitando a configuração e o monitoramento da comunicação entre serviços. Com sua arquitetura extensível e suporte a diversos protocolos, o Skupper se adapta a diferentes cenários de integração, atendendo às necessidades de projetos de todos os portes. Resumo Neste workshop, você aprendeu como usar o Red Hat Service Interconnect (Skupper) para conectar um banco de dados a um cluster Kubernetes, permitindo que aplicações distribuídas se comuniquem de forma eficiente e segura. Com o Skupper, você pode simplificar a integração de serviços em ambientes heterogêneos, facilitando o desenvolvimento e a implantação de aplicações modernas. Esperamos que este workshop tenha sido útil e que você possa aplicar esses conhecimentos em seus próprios projetos. Obrigado por participar!" }, { "title": "Using Skupper and OpenShift AI/ML to Prevent Insurance Fraud", "url": "/posts/AI-com-skupper-para-previnir-fraudes/", "categories": "AI, Skupper", "tags": "ai, machine-learning, skupper, openshift, fraud-detection, insurance, networking, security", "date": "2024-06-17 00:00:00 -0300", "content": "Description This workshop demonstrates how to use Skupper to connect local data services to cloud-based AI/ML environments. The workshop includes a Go application in a podman container that exposes internal data for Skupper connection. The AI/ML model training is performed in an OpenShift AI cluster on AWS using Openshift AI/ML services. Disclaimer This lab uses the example from the AI/ML Workshop created by the Red Hat AI Services team. The original workshop is available on GitHub and includes all the necessary information to run the lab. The lab was adapted to use Skupper to connect the local data services to the cloud-based AI/ML environment. In order to faciliate the execution, for those who have access to the demo.redhat.com environment, you can start the lab by clicking here. If you don’t have access to the demo environment, you can follow the steps at the gitub repository mentioned above. References Red Hat AI/ML Workshop GO Application to expose internal data Modified examples for the workshop The Developers Conference Workshop Repository Skupper Workshop Overview This lab demonstrates how AI/ML technologies can solve a business problem. The information, code, and techniques presented illustrate a prototype solution. Key steps include: Storing raw claim data within the company. Using a Go application in a podman container to expose internal data for Skupper connection. Setting up AI/ML model training in an OpenShift AI cluster on AWS. Connecting local data to cloud-based AI/ML services using Skupper. Skupper Role Skupper provides secure, efficient connections between different environments. In this workshop, it connects local data services containing sensitive insurance claim information to a cloud-based AI/ML environment. This secure connection allows remote data access and processing while maintaining data integrity and security. Process Structure Context Connection and Setup LLM for Text Summarization LLM for Information Extraction LLM for Sentiment Analysis Scenario We are a multinational insurance company undergoing digital transformation. A small team has analyzed the claims process and proposed improvements. The goal is to integrate the claims processing solution with text analysis using our API in a Kubernetes cluster on AWS. Challenges Using Skupper to Ensure Data Security and Integrity Maintaining data integrity and security: Skupper encrypts all data traffic, ensuring sensitive data protection during transmission. Processing emails with OpenShift AI in the on-premises datacenter. Keeping applications with sensitive data within the company. Ensuring secure connections between data services and datacenters. Prototyping Work Examples Using an LLM for Text Summarization An LLM can summarize long emails, allowing insurance adjusters to quickly understand key details. Using an LLM for Information Extraction An LLM extracts key information from emails and automatically populates structured forms. Using an LLM for Sentiment Analysis An LLM identifies customer sentiment, allowing for prompt action based on text tone. How to Use LLMs? Notebook for using LLM Notebook for text summarization with LLM Notebook for information extraction with LLM Notebook for comparing LLM models Part 2: Hands-On Activities Install Skupper binary Install Skupper locally Install Skupper on the OpenShift Cluster Linking the sites Run the application inside the podman site and expose the service Execute the workshop with modified examples Steps Installing the Skupper binary curl https://skupper.io/install.sh | sh Installing Skupper on the podman site export SKUPPER_PLATFORM=podman podman network create skupper skupper init --ingress none Install Skupper on the OpenShift Cluster skupper init --enable-console --enable-flow-collector --console-user admin --console-password admin Linking the sites Creating the token on the most exposed cluster skupper token create /tmp/insurance-claim Linking the podman site to the most exposed cluster skupper link create /tmp/insurance-claim --name ai Running the application inside the podman site and exposing the service podman run -d --network skupper -p 8080:8080 -v /home/rzago/Code/go-flp/data:/app/data --name insurance-claim-data quay.io/rzago/insurance-claim-data:latest skupper service create backend 8080 skupper service bind backend host insurance-claim-data --target-port 8080 skupper service create backend 8080 Successful Connection Final Topology Testing the connection to the podman site service from the OpenShift cluster oc exec deploy/skupper-router -c router -- curl http://backend:8080/claim/claim1.json Next Steps Now you can continue with the workshop until generating the sentiments of the emails. Conclusion This workshop demonstrates how to use Skupper to connect local data services to cloud-based AI/ML environments. The workshop includes a Go application in a podman container that exposes internal data for Skupper connection. The AI/ML model training is performed in an OpenShift AI cluster on AWS using Openshift AI/ML services." }, { "title": "TemPy: An IoT Architecture with Raspberry Pi and Skupper", "url": "/posts/tempi-com-skupper-e-grafana/", "categories": "skupper, raspberry, grafana, opensource, english", "tags": "skupper, raspberry-pi, iot, grafana, prometheus, opensource, networking, monitoring, temperature-sensor", "date": "2024-02-15 00:00:00 -0300", "content": "Description This project is a proof of concept for an IoT architecture using a Raspberry Pi and a temperature sensor that exposes the temperature data through a REST API. Along with the REST API, there is a cloud integration with any cloud provider using Skupper that enables the data to be visualized in a Grafana dashboard. Clone the repository and follow the instructions to run the project. https://github.com/rafaelvzago/skupper-tempy git clone https://github.com/rafaelvzago/skupper-tempy.git Table of Contents Hardware Raspberry Configuration Temperature Capture Skupper Role Connection to a cluster using skupper and storage data into prometheus Prometheus Grafana Repository Architecture The architecture of the project can be divided into the following parts: Hardware This part involves the physical components used in the project, such as the Raspberry Pi and the temperature sensor. Raspberry Pi 3 Model B+ Raspberry Pi 3 Model B+ DS18B20 Temperature Sensor DS18B20 Temperature Sensor 4.7kΩ Resistor 4.7kΩ Resistor Breadboard Breadboard Jumper Wires Jumper Wires Raspberry Configuration This part focuses on the setup and configuration of the Raspberry Pi, including installing the necessary software and libraries. Ubuntu 23.04 server for Raspberry Pi Ubuntu Installation GoLang 1.18+ GoLang Installation Skupper Main Skupper Installation Podman &gt; 4.3 Podman Installation Temperature Capture Credits: Raspberry Pi DS18B20 Temperature Sensor Tutorial In this part, the temperature sensor is connected to the Raspberry Pi, and the code for capturing temperature readings is implemented. Configuration: Raspberry Pi GPIO Pins: Pin 1 (3.3V) is connected to the VDD pin of the DS18B20. Pin 7 (GPIO 4) is connected to the DQ pin of the DS18B20. Pin 9 (GND) is connected to the GND pin of the DS18B20. DS18B20: The VDD pin is powered by 3.3V from the Raspberry Pi. The DQ pin is connected to GPIO 4 with a pull-up resistor. The GND pin is grounded to the Raspberry Pi. Connections: A 4.7kΩ pull-up resistor (R1) is placed between the VDD and DQ lines. The VDD line from the DS18B20 is connected to a red wire representing 3.3V from the Raspberry Pi. The DQ line is connected to a white wire representing data and is connected to GPIO 4 on the Raspberry Pi. The GND line is connected to a black wire representing ground from the Raspberry Pi. Functionality: The DS18B20 temperature sensor reports temperature data through the 1-Wire interface, which requires only one data line (and ground) for communication with the Raspberry Pi. The pull-up resistor is necessary for the 1-Wire protocol used by the DS18B20 to function correctly. REST API: To expose the temperature data, a REST API is implemented using GoLang. The API is used to capture the temperature data and expose it to the cloud provider. go build tempy/tempy.go Find a way to run the tempy binary on the Raspberry Pi, for this example I will use a simple nohup command to run the tempy binary in the background. nohup ./tempy &amp; The REST API is exposed on port 5000/temperature, and the temperature data can be accessed using the following command: curl localhost:5000/temperature Skupper Role We will use Skupper to establish communication between the Raspberry Pi and the cloud provider, and to expose the temperature data to the cloud. This part covers the setup and configuration of Skupper. Skupper is a layer 7 service interconnect that enables secure communication across Kubernetes clusters, including network and application layer protocols. Skupper is designed to connect services that are running on different infrastructure, and it is based on the idea of a service bus. Skupper In this example we will be using skupper gateway to expose the temperature data to the cloud, for this we will need to have a skupper site running on the cloud and a skupper gateway running on the Raspberry Pi. Skupper gateway is a component that allows non-kubernetes services to be exposed to the skupper network, in this case we will use the skupper gateway to expose the temperature data to the cloud. Skupper site: A namespace running skupper, for this example we will borrow the prometheus service to store the temperature data, so we will init skupper on the cluster with the following command: skupper init --site-name site1 --enable-console --enable-flow-collector Skuper gateway on the Raspberry Pi: To expose the temperature data to the cloud, we will use a skupper gateway to expose the temperature data to the cloud, for this we will use the following command: skupper gateway expose tempy localhost 5000 --type podman Connection to a cluster using skupper and storage data into prometheus The temperature data captured by the Raspberry Pi is stored in the cloud using the chosen cloud provider. This part explains how the data is stored and managed. For this example, we will deploy a prometheus service to store the temperature data, and a prometheus-adapter to scrape the temperature data from the REST API and store it in the prometheus service. In order to facilitate the prometheus role, we will configure the prometheus service discovery to scrape the temperature data from the prometheus-adapter or any other service labeled as app=metric. with this approach, we can easily add more temperature sensors to the architecture and the prometheus service will automatically scrape the temperature data from the new sensors. In order to achive this, the service will be labeled as app=metric, and the prometheus-adapter will add the temperature data to the service, so the prometheus service will scrape the temperature data from the prometheus-adapter. apiVersion: v1 kind: Service metadata: name: tempy-prometheus-adapter-service spec: type: ClusterIP selector: app: metrics Prometheus Adapter: Build the TemPy prometheus-adapter image: podman build -t quay.io/YOUR-USER/tempy-prometheus-adapter:0.1 -f prometheus-adapter/Dockerfile-TempyPrometheusAdapter . Push the image to the quay.io registry: podman push quay.io/YOUR-USER/tempy-prometheus-adapter:0.1 Deploy the prometheus-adapter: kubectl apply -f prometheus-adapter/TempyPrometheusAdapter-deployment.yaml Expose the prometheus-adapter: kubectl apply -f prometheus-adapter/TempyPrometheusAdapter-service.yaml Verify the prometheus-adapter is running: kubectl run -i --tty --rm curl-pod --image=curlimages/curl -- sh curl tempy-prometheus-adapter:9090/metrics ... # TYPE temperature_celsius gauge temperature_celsius 19.81 # HELP temperature_fahrenheit Current temperature in Fahrenheit # TYPE temperature_fahrenheit gauge temperature_fahrenheit 67.66 Check the prometheus-adapter service to check if the labels are being added to the service: kubectl get svc tempy-prometheus-adapter-service -o wide NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR tempy-prometheus-adapter-service ClusterIP 10.43.154.250 &lt;none&gt; 9090/TCP 11h app=metrics Prometheus The temperature data is stored in a prometheus service. This part covers the setup and configuration of the prometheus service. We need to persist the data, so we will use a PVC, in my case I will use the longhorn storage class, but you can use any storage class that you have available in your cluster. This is a part of the prometheus configuration file, it is configured to scrape the temperature data from any service labeled as app=metrics, so the prometheus service will scrape the temperature data from the prometheus-adapter. Note that this configuration will only look for services labeled as app=metrics in the skupper-pi namespace, so if you are using a different namespace, you will need to change the configuration file accordingly. ... - job_name: 'metrics-targets' scrape_interval: 5s kubernetes_sd_configs: - role: service namespaces: names: ['skupper-pi'] relabel_configs: - source_labels: [__meta_kubernetes_service_label_app] regex: metrics action: keep ... Create prometheus PVC: kubectl apply -f prometheus/prometheus-pvc.yaml Create prometheus deployment: kubectl apply -f prometheus/prometheus-deployment.yaml Configuring the prometheus service discovery to scrape the temperature data from any service labeled as app=metrics: kubectl apply -f prometheus/prometheus-cm.yaml Deploy the prometheus: kubectl apply -f prometheus/prometheus-deployment.yaml Create prometheus service: kubectl apply -f prometheus/prometheus-service.yaml Verify the prometheus is running, from this point on, the prometheus service should be scraping the temperature data from the prometheus-adapter or any other service labeled as app=metrics, let’s query all the services discovered by prometheus: kubectl run -i --tty --rm curl-pod --image=curlimages/curl -- sh -c 'curl -G --data-urlencode \"query=up\" http://prometheus:9090/api/v1/query' | jq . If you don't see a command prompt, try pressing enter. warning: couldn't attach to pod/curl-pod, falling back to streaming logs: Internal error occurred: error attaching to container: container is in CONTAINER_EXITED state { \"status\": \"success\", \"data\": { \"resultType\": \"vector\", \"result\": [ { \"metric\": { \"__name__\": \"up\", \"instance\": \"localhost:9090\", \"job\": \"prometheus\" }, \"value\": [ 1708523706.121, \"1\" ] }, { \"metric\": { \"__name__\": \"up\", \"instance\": \"promock.skupper-pi.svc:80\", \"job\": \"metrics-targets\" }, \"value\": [ 1708523706.121, \"1\" ] }, { \"metric\": { \"__name__\": \"up\", \"instance\": \"tempy-prometheus-adapter-service.skupper-pi.svc:9090\", \"job\": \"metrics-targets\" }, \"value\": [ 1708523706.121, \"1\" ] } ] } } ... Grafana The stored temperature data is visualized in a Grafana dashboard. This part covers the setup and configuration of the dashboard. To visualize the temperature data, we will use a Grafana dashboard. The dashboard is configured to scrape the temperature data from the prometheus service and visualize it in a graph. The grafana deployment is done using the following command: For the data persistence, we will use a PVC to store the grafana data, in my case I have longhorn installed on my cluster, so I will use it to store the grafana data, but you can use any other storage class that you have available on your cluster. Create grafana PVC: kubectl apply -f grafana/grafana-pvc.yaml Create grafana deployment: kubectl apply -f grafana/grafana-deployment.yaml Create grafana service: kubectl apply -f grafana/grafana-service.yaml Important: My cluster is configured to use the ingress controller, so I have to create an ingress to expose the grafana service, if your cluster is not configured to use the ingress controller, you will need either to expose the grafana service using a nodeport or a loadbalancer. Create a data source connection in grafana that points to the prometheus service: http://skupper-prometheus:9090 Import the grafana dashboard: grafana/dashboard.json FINALY, you should be able to visualize the temperature data in the grafana dashboard. Repository The complete code for the project can be found in the following GitHub repository: TemPy" }, { "title": "Carreira com Software Livre - O que é e como começar?", "url": "/posts/carreira-com-software-livre/", "categories": "carreira, opensource", "tags": "carreira, opensource, software-livre, linux, apache, cncf, contribuicao, licencas", "date": "2023-12-07 00:00:00 -0300", "content": "Carreira com Software Livre - O que é e como começar? Introdução Software Livre é um movimento que tem como objetivo promover a liberdade de uso, estudo, modificação e distribuição de software. O movimento do Software Livre é baseado em quatro liberdades essenciais: A liberdade de executar o programa, para qualquer propósito (liberdade nº 0). A liberdade de estudar como o programa funciona e adaptá-lo para as suas necessidades (liberdade nº 1). O acesso ao código-fonte é um pré-requisito para esta liberdade. A liberdade de redistribuir cópias de modo que você possa ajudar ao seu próximo (liberdade nº 2). A liberdade de aperfeiçoar o programa, e liberar os seus aperfeiçoamentos, de modo que toda a comunidade se beneficie (liberdade nº 3). O acesso ao código-fonte é um pré-requisito para esta liberdade. Tipos de licenças para software livre Existem diversos tipos de licenças para software livre, sendo as mais populares a GPL, LGPL, MIT, Apache, BSD e a Mozilla. Cada uma dessas licenças possui suas próprias características e restrições, porém, todas elas garantem as quatro liberdades essenciais do movimento do Software Livre. GPL: A GPL é uma licença copyleft, o que significa que qualquer software que utilize uma biblioteca licenciada sob a GPL também deve ser licenciado sob a GPL. A GPL é uma licença muito popular entre os desenvolvedores de software livre, pois garante que o software permaneça livre e aberto para todos. LGPL: A LGPL é uma licença copyleft, o que significa que qualquer software que utilize uma biblioteca licenciada sob a LGPL também deve ser licenciado sob a LGPL. A LGPL é uma licença muito popular entre os desenvolvedores de software livre, pois garante que o software permaneça livre e aberto para todos. MIT: A MIT é uma licença permissiva, o que significa que qualquer software que utilize uma biblioteca licenciada sob a MIT pode ser licenciado sob qualquer outra licença. A MIT é uma licença muito popular entre os desenvolvedores de software livre, pois garante que o software permaneça livre e aberto para todos. Apache: A Apache é uma licença permissiva, o que significa que qualquer software que utilize uma biblioteca licenciada sob a Apache pode ser licenciado sob qualquer outra licença. A Apache é uma licença muito popular entre os desenvolvedores de software livre, pois garante que o software permaneça livre e aberto para todos. BSD: A BSD é uma licença permissiva, o que significa que qualquer software que utilize uma biblioteca licenciada sob a BSD pode ser licenciado sob qualquer outra licença. A BSD é uma licença muito popular entre os desenvolvedores de software livre, pois garante que o software permaneça livre e aberto para todos. Mozilla: A Mozilla é uma licença permissiva, o que significa que qualquer software que utilize uma biblioteca licenciada sob a Mozilla pode ser licenciado sob qualquer outra licença. A Mozilla é uma licença muito popular entre os desenvolvedores de software livre, pois garante que o software permaneça livre e aberto para todos. Como começar a contribuir com software livre? Existem diversas formas de contribuir com software livre, sendo as mais populares a contribuição de código, a contribuição de documentação e a contribuição de tradução. Cada uma dessas formas de contribuição possui suas próprias características e restrições, porém, todas elas garantem as quatro liberdades essenciais do movimento do Software Livre. A Importância do Software Livre na Indústria de Tecnologia O Software Livre é um movimento que tem como objetivo promover a liberdade de uso, estudo, modificação e distribuição de software. Hoje existem vários projetos que são pilares da indústria e são mantidos por comunidades de desenvolvedores que trabalham de forma voluntária. Esses projetos são essenciais para o desenvolvimento de novas tecnologias e para a evolução da indústria de tecnologia como um todo. Por isso, é importante que os desenvolvedores se envolvam com o movimento do Software Livre e contribuam com projetos que são essenciais para a indústria de tecnologia. Exemplos de software livre que são essenciais para a indústria de tecnologia: Linux: O Linux é um sistema operacional de código aberto que é utilizado por milhões de pessoas em todo o mundo. https://www.kernel.org/. Apache: O Apache é um servidor web de código aberto que é utilizado por milhões de pessoas em todo o mundo. https://httpd.apache.org/. MariaDB: O MariaDB é um sistema de gerenciamento de banco de dados de código aberto derivado do MySQL. https://mariadb.org/. Docker: O Docker é uma plataforma de código aberto que permite que os desenvolvedores empacotem seus aplicativos em contêineres. https://www.docker.com/. Kubernetes: O Kubernetes é uma plataforma de código aberto que permite que os desenvolvedores gerenciem seus aplicativos em contêineres. https://kubernetes.io/. Ansible: O Ansible é uma ferramenta de código aberto que permite que os desenvolvedores automatizem a implantação de seus aplicativos. https://www.ansible.com/. Jenkins: O Jenkins é uma ferramenta de código aberto que permite que os desenvolvedores automatizem a construção e o teste de seus aplicativos. https://www.jenkins.io/. Git: O Git é uma ferramenta de código aberto que permite que os desenvolvedores controlem as versões de seus aplicativos. https://git-scm.com/. Software livre em números: Atualmente existem mais de 1.000.000 de projetos de software livre no GitHub. O GitHub é a maior plataforma de desenvolvimento de software livre do mundo. GitHub O Linux é o sistema operacional mais utilizado no mundo. Linux Existem mais de projetos 1861 projetos na CNCF. Tanto projetos graduados, incubados, sandbox e arquivados. CNCF Existem mais de 300 projetos na Apache. Apache CNCF - Cloud Native Computing Foundation CNCF (Cloud Native Computing Foundation): Uma organização sem fins lucrativos. Objetivo: Promover o desenvolvimento de software livre para a nuvem. Características: Foco em Cloud Native: Concentra-se em tecnologias que empoderam sistemas escaláveis, resilientes e ágeis baseados em contêineres. Comunidade Aberta e Colaborativa: Encoraja a colaboração e contribuição abertas entre membros da indústria, desenvolvedores e usuários finais. Padrões e Práticas de Governança: Estabelece padrões e melhores práticas para garantir interoperabilidade e eficiência. Apoio à Inovação e Sustentabilidade: Fomenta a inovação e sustentabilidade de projetos e comunidades de software livre. Eventos e Educação: Organiza eventos, webinars e programas educacionais para promover conhecimento e colaboração na comunidade cloud native. Projetos Chave: Kubernetes: Um sistema de orquestração de contêineres. Prometheus: Sistema de monitoramento e alerta. Envoy: Um proxy de serviço de código aberto. Links: Site Oficial Kubernetes Prometheus Envoy CNCF Projects Apache Software Foundation ASF (Apache Software Foundation): Uma organização sem fins lucrativos. Objetivo: O objetivo da ASF é fornecer software livre para o público em geral. Principais Projetos da Apache Software Foundation: Apache Hadoop: Framework para processamento distribuído de grandes conjuntos de dados. Apache Kafka: Plataforma de streaming distribuído para construção de pipelines de dados em tempo real. Apache Cassandra: Banco de dados distribuído para lidar com grandes quantidades de dados. Apache Spark: Motor de análise unificado para processamento de dados em larga escala. Apache Lucene: Biblioteca de software para recuperação de informações e pesquisa de texto completo. Apache Tomcat: Contêiner de servlets para aplicações web Java. Apache Maven: Ferramenta de automação de compilação para projetos Java. Apache HBase: Banco de dados não relacional distribuído para grandes conjuntos de dados. Apache Flink: Framework e mecanismo de processamento de fluxo distribuído. Apache Airflow: Plataforma para programar, coordenar e monitorar fluxos de trabalho. Links: Site Oficial Apache Hadoop Apache Kafka Apache Cassandra Apache Spark Apache Lucene Apache Tomcat Apache Maven Apache HBase Apache Flink Apache Airflow Linux Foundation A Linux Foundation é conhecida por seu apoio a vários projetos importantes de código aberto, incluindo o Linux Kernel, Kubernetes, Hyperledger e Node.js. Eles se concentram em fornecer um lar neutro e apoio para a colaboração em tecnologias de código aberto, priorizando inovação, inclusão e desenvolvimento sustentável. Para obter informações detalhadas e uma lista completa dos projetos, recomendo visitar diretamente o site da Linux Foundation: Linux Foundation Projects. Principais Projetos da Linux Foundation: CNCF: Cloud Native Computing Foundation. LF AI: Linux Foundation Artificial Intelligence. LF Edge: Linux Foundation Edge. LF Energy: Linux Foundation Energy. LF Public Health: Linux Foundation Public Health. OpenJS Foundation: OpenJS Foundation. RISC-V: RISC-V. Links: Site Oficial Linux Foundation Projects Resumo Esse artigo discute a carreira em Software Livre, enfatizando a importância das quatro liberdades essenciais do movimento: executar, estudar, redistribuir e aperfeiçoar programas. Ele explora diferentes tipos de licenças, como GPL, LGPL, MIT, Apache, BSD e Mozilla, destacando suas características únicas. O texto sugere formas de contribuir com software livre, incluindo código, documentação e tradução. Destaca a relevância do Software Livre na indústria de tecnologia, citando exemplos como Linux, Apache e Docker. Além disso, menciona projetos e iniciativas de organizações como CNCF, Apache Software Foundation e Linux Foundation, fornecendo links úteis e informações sobre seus projetos e objetivos." }, { "title": "Quem sou eu?", "url": "/posts/quem-sou-eu/", "categories": "sobre, off-topic", "tags": "sobre, pessoal, rafael-zago, devops, redhat, palestrante, instrutor", "date": "2023-12-04 00:00:00 -0300", "content": "Apresentação Qué pasa? Olá! Não me leve muito a sério, ok? Eu criei esse espaço para compartilhar um pouco do conhecimento que eu tenho e que 90% dele, eu ganhei na internet. Então já passou da hora de devolver um pouco para o mundo. Hoje sou senior software automation engineer na Red Hat, mas já passei por algumas empresas (pequenas e grandes) fazendo de tudo um pouco: DevOps de coração e SysAdmin de vocação. Criando métodos de aprendizagem e estruturando treinamentos. Trabalhei, por muitos anos, com suporte de aplicações de vários tamanhos e complexidade. Sou instrutor da Caelum/Alura. Sou padrinho do carinha mais dahora da terra! Sem clubismo… Comunidades Sou devops de coração e também membro da organização do DevOpsDays SP Organização do DevOpsDays 2020 Abaixo estão as poucas contribuições que já fiz: Podcasts Hipsters Ponto Tech #239 DNE 224 - Trabalhar para Gringa Talks/Palestras Uma estratégia upstream e downstream para entrega contínua - Mini DebConf Brasília!, 2023. Skupper, a cloud híbrida com 3 comandos - DevOpsDays Fortaleza, 2022. Take out the rust, transformando time com responsabilidade - DevOpsDays São Paulo, 2019. Desenvolvendo Aplicações na Nuvem: Uma Abordagem Prática - LinuxDay Limeira, 2014 Palestra Linux 101 - Unisal Campinas, 2018 IHC and Security Talk - Unisal Campinas 2018 Vídeos O que é Openshift? - Alura" }, { "title": "CI/CD Upstream vs Downstream", "url": "/posts/ci-cd-downstream-upstream/", "categories": "pipeline", "tags": "ci-cd, pipeline, devops, automation, upstream-downstream, redhat, skupper", "date": "2023-06-02 00:00:00 -0300", "content": "Integrando Skupper com Skupper: Uma abordagem upstream e downstream Quando se trata de desenvolvimento de software, integrar projetos upstream e downstream pode ser um desafio. Neste artigo, vamos explorar uma abordagem eficaz para integrar o Skupper e o Skupper, utilizando ferramentas populares para cada lado do desenvolvimento. Introdução O Skupper é um projeto de software livre que fornece uma solução de rede de serviço para Kubernetes. O Skupper é desenvolvido pela Red Hat e está disponível sob a licença Apache 2.0. Essa é a versão upstream do Skupper, o que significa que é a versão que está sendo desenvolvida ativamente pela Red Hat. Para os usuários que desejam implantar o Skupper em um ambiente de produção, a Red Hat oferece o Skupper, uma versão comercial do Skupper que é fornecida pela Red Hat sob o nome de Red Hat AMQ Interconnect. Essa é a versão downstream do Skupper, o que significa que é a versão portada para o Red Hat Enterprise Linux e como Opera com o Red Hat OpenShift. Apesar de serem projetos diferentes, o Skupper e o Skupper compartilham uma base de código comum e, portanto, é importante que as alterações feitas no Skupper sejam integradas ao RHSI (Red HaT Application Interconnect). Para isso, é necessário estabelecer um fluxo de trabalho eficiente que permita a integração contínua entre o Skupper e o RHSI. Definindo o ambiente e as ferramentas Antes de começarmos, vamos configurar nosso ambiente de desenvolvimento. Para o lado downstream, faremos uso das seguintes ferramentas: Jira: uma ferramenta de gerenciamento de projetos que nos ajudará a rastrear e organizar as tarefas relacionadas ao desenvolvimento downstream. Confluence: uma plataforma de colaboração que usaremos para documentar informações importantes sobre o projeto. Git: um sistema de controle de versão que nos permitirá gerenciar nosso código fonte e colaborar com outros desenvolvedores. Quay.io: um registro de contêineres que nos ajudará a armazenar e distribuir nossas imagens de contêineres. Jenkins: uma ferramenta de automação de integração contínua que nos permitirá construir, testar e implantar nosso software de forma automatizada. Por outro lado, para o desenvolvimento upstream, faremos uso das seguintes ferramentas: GitHub Issues: um recurso do GitHub que nos ajudará a rastrear e gerenciar problemas e solicitações de recursos relacionados ao desenvolvimento upstream. CircleCI: uma plataforma de integração contínua que nos permitirá construir, testar e validar nosso código de forma automatizada. Fluxo de trabalho Agora que nosso ambiente está configurado, vamos explorar um fluxo de trabalho básico para a integração contínua entre o Skupper e o Skupper. Desenvolvimento Upstream Utilize o GitHub Issues para rastrear e gerenciar problemas e solicitações de recursos. Faça uso do CircleCI para construir e testar o código do Skupper de forma automatizada. Integração Upstream-Downstream Após o desenvolvimento upstream estar pronto, abra uma solicitação de pull no repositório do Skupper. Uma vez que a solicitação de pull seja aprovada, uma nova versão do Skupper é criada e publicada no Quay.io. Desenvolvimento Downstream Utilize o Jira para criar tarefas relacionadas às funcionalidades downstream. Utilize o Git para clonar o código fonte do Skupper e iniciar o desenvolvimento downstream. Use o Jenkins para automatizar a construção, teste e implantação do Skupper. Integração Downstream-Upstream Quando necessário, faça alterações no código do Skupper e abra uma solicitação de pull no repositório. Após a aprovação da solicitação de pull, uma nova versão do Skupper é publicada. Exemplo prático Para ilustrar o fluxo de trabalho descrito acima, vamos considerar um cenário em que estamos adicionando suporte para um novo protocolo de comunicação no Skupper e integrando essa funcionalidade ao Skupper. Desenvolvimento Upstream Abra um problema no GitHub Issues para rastrear a solicitação de suporte ao novo protocolo. Escreva o código necessário para adicionar o suporte no Skupper. Utilize o CircleCI para construir e testar o código automaticamente. Integração Upstream-Downstream Abra uma solicitação de pull no repositório do Skupper para incorporar as alterações. Após a aprovação da solicitação de pull, uma nova versão do Skupper é publicada no Quay.io. Desenvolvimento Downstream No Jira, crie uma tarefa para adicionar suporte ao novo protocolo no Skupper. Clone o repositório do Skupper usando o Git. Adicione o suporte ao novo protocolo no código do Skupper. Use o Jenkins para automatizar a construção, teste e implantação do Skupper com as novas alterações. Integração Downstream-Upstream Se necessário, faça alterações adicionais no código do Skupper e abra uma solicitação de pull. Após a aprovação da solicitação de pull, uma nova versão do Skupper é publicada. Conclusão A integração contínua entre o Skupper e o Skupper é essencial para garantir que as atualizações do software cheguem aos usuários de forma eficiente, mantendo a viabilidade comercial. Utilizando ferramentas como Jira, Confluence, Git, Quay.io, Jenkins, GitHub Issues e CircleCI, é possível estabelecer um fluxo de trabalho robusto e automatizado que agiliza o desenvolvimento upstream e downstream, permitindo a entrega contínua de software de alta qualidade. Esperamos que este artigo tenha fornecido insights valiosos sobre a integração upstream e downstream e tenha demonstrado como essas ferramentas podem ser usadas em conjunto para um processo de desenvolvimento mais eficiente. Obrigado por ler e continue explorando as possibilidades de integração contínua entre projetos de software livre e produtos comerciais!" }, { "title": "Criando uma rede de aplicativos multicloud com Skupper", "url": "/posts/multicloud-com-skupper/", "categories": "cloud, k8s, skupper", "tags": "kubernetes, skupper, cloud, redhat, multicloud, networking, devops", "date": "2022-10-01 00:00:00 -0300", "content": "Referências e tecnologias utilizadas: https://skupper.io https://minikube.sigs.k8s.io Repositório com os Códigos Qpid-dispatch ActiveMQ Kubectl Kubernetes MTLS Open-source Skupper-router Skupper Ferramentas Um computador com o minikube [2] instalado; Um terminal para executar os comandos; kubectl &gt; 1.15 [6] ou mais nova. Descrição da solução O Skupper [1] é uma ferramenta que permite conectar dois ou mais ambientes de cloud de uma maneira não intrusiva e segura. Tais ambientes podem ser de diferentes provedores de serviço em nuvem como: AWS, GCP, AZURE entre outras, e, inclusive, clusters kubernetes nativos. tl;dr Para quem está começando com cloud: O Skupper é uma ferramenta que permite conectar diferentes ambientes de computação em nuvem de maneira segura e sem complicações. Imagine que você tem duas salas diferentes, cada uma com seu próprio conjunto de ferramentas. Skupper é como uma porta segura que permite que essas salas “conversem” entre si, compartilhando ferramentas conforme necessário. Isso é útil quando você tem diferentes partes de um aplicativo rodando em diferentes lugares, mas elas precisam trabalhar juntas como se estivessem no mesmo lugar. Para quem tem algum conhecimento de cloud: O Skupper é uma solução de rede de serviço para Kubernetes que permite a comunicação segura e fácil entre clusters. Ele cria uma camada de rede virtual que conecta pods em diferentes clusters como se estivessem na mesma rede local. Isso é feito sem a necessidade de privilégios de administrador do cluster e sem a necessidade de expor serviços à Internet pública. Além disso, o Skupper não é intrusivo com sua aplicação, pois não cria side-cars ou outros containers dentro dos Pods. Ele é open-source e oferece criptografia de ponta a ponta usando certificados digitais. Para quem gosta de escovar bits: O Skupper pode ser dividido em duas partes: o skupper-router e o control-plane chamado skupper-service-controller. O skupper-router é um roteador de rede de serviço baseado no Qpid-dispatch [4] e no ActiveMQ [5]. O skupper-service-controller é um controlador Kubernetes que gerencia o skupper-router e fornece uma API para configurar e gerenciar a rede de serviço. Existem outros containers que são usados para configurar e gerenciar o skupper-router, mas eles são apenas auxiliares e não são necessários para o funcionamento do Skupper. Mas isso vai ficar para outro post. Solução Esse exemplo consiste em dois serviços: 1. Frontend Um serviço de backend que expõe um endpoint /api/hello. Que tem como resposta Oi, &lt;seu-nome&gt;. Eu sou &lt;meu-nome&gt; (&lt;nome-pod&gt;). O deploy será feito no namespace config_oeste; 2. Backend Um serviço de frontend que expõe um endpoint /api/hello que faz uma chamada para o serviço de backend e retorna a resposta, mas nesse caso o serviço esta rodando em outro namespace chama config_leste, este por sua vez pode estar em outro cluster ou namespace. Por que usar o Skupper? Com o Skupper, você pode colocar o back-end em um cluster e o front-end em outro e manter a conectividade entre os dois serviços sem expor o back-end à Internet pública. Detalhes: Não é necessário ter privilégios de administrador do cluster, já que a solução é no nível do namespace; Não é intrusivo com a sua aplicação, pois não cria side-cars ou outros containers dentro dos Pods; É open-source; Você pode conectar, em seu cluster, serviços externos como: Bancos de dados, aplicações legadas e ainda de alta criticidade; Criptografado de ponta a ponta usando certificados digitais; Baixa curva de aprendizagem. MTLS [9] por padrão. MTLS é um protocolo de segurança que garante que a comunicação entre dois pontos seja feita de maneira segura e criptografada. Utilização de certificados próprios caso necessário, ou seja, você pode usar os certificados da sua empresa ou gerar novos certificados para o Skupper ( que é o padrão e são criados automaticamente). Agora vamos preparar nosso ambiente de teste, que consiste no seguinte: Um serviço de backend que está rodando em um namespace que vai prover a lógica para outro serviço de frontend que obviamente está e outro namespace*. Nesse caso, cada serviço está rodando em namespaces diferentes, mas o mesmo exemplo pode (e deve) ser testado com provedores diferentes. 1. Nesse exemplo vamos utilizar dois namespaces chamados: 1.1. config_oeste onde ficará o frontend. Lembre-se de abrir uma aba do seu terminal para cada namespace export KUBECONFIG=~/.kube/config-oeste 1.2. config_leste onde ficará o backend agora, no outro terminal: export KUBECONFIG=~/.kube/config-leste Obs: Você pode usar o nome que quiser para os namespaces, mas lembre-se de usar o mesmo nome no arquivo de configuração do skupper. 2. Configurando cada namespace: 2.1.config_oeste: kubectl create namespace oeste kubectl config set-context --current --namespace oeste 2.2.config_leste: kubectl create namespace leste kubectl config set-context --current --namespace leste 3. Instalando o Skupper: Você possui algumas maneiras de instalar o skupper, como por exemplo: Compilar a partir do repositório Fazer o download do executável direto do repositório [1] do projeto Usar o script de instalação disponibilizado pelo site skupper.io e vamos utilizar esse método, por ser mais fácil de fazer e você não precisará se preocupar com dependências. 4. Instaçação do CLI do Skupper: curl https://skupper.io/install.sh | sh 5. Iniciando o skupper nos dois namespaces: 5.1.config_oeste: skupper init 5.2.config_leste: skupper init 6. Conectando os namespaces: A criação de um link requer o uso de dois comandos skupper em conjunto: skupper token create e skupper link create. O comando skupper token create gera um token secreto que significa permissão para criar um link. O token também carrega os detalhes do link. Em seguida, em um namespace remoto, o comando skupper link create usa o token para criar um link para o namespace que o gerou. Nota: O token de link é realmente um segredo. Qualquer pessoa que tenha o token pode vincular ao seu namespace. Certifique-se de que apenas aqueles em quem você confia tenham acesso a ele. Porém sua utilização pode ser controlada por número de usos e tempo de vida. Veja a documentação do skupper [1] para mais detalhes. 6.1. Criando o token no namespace config_oeste: skupper token create ~/secret.token Token written to ~/secret.token 6.2. Fazendo o link no namespace config_leste ao namespace config_oeste com o token gerado: skupper link create ~/secret.token 7. Fazendo o deploy do frontend e do backend: 7.1. Applicando o YAML para fazer o deploy do frontend no namespace config_oeste: kubectl create deployment frontend --image quay.io/skupper/hello-world-frontend deployment.apps/frontend created 7.2. Applicando o YAML para fazer o deploy do backend no namespace config_leste: kubectl create deployment backend --image quay.io/skupper/hello-world-backend --replicas 3 deployment.apps/backend created 8. Expondo os serviços de backend: Agora que os serviços estão rodando, vamos expor os serviços para que possamos acessá-los. Nesse caso, vamos expor o serviço de backend para que o frontend possa acessá-lo, independente de onde ele esteja rodando. 8.1. Expondo o serviço de backend no namespace config_leste: skupper expose deployment/backend --port 8080 deployment backend exposed as backend 9. Expondo os serviços de frontend: Agora que os serviços estão rodando, vamos expor os serviços para que possamos acessá-los. Nesse caso, vamos expor o serviço de frontend para que possamos acessá-lo, independente de onde ele esteja rodando. 9.1. Expondo o serviço de frontend no namespace config_oeste: kubectl expose deployment frontend --port 8080 --type LoadBalancer service/frontend exposed 10. Testando a aplicação: Agora que os serviços estão rodando, vamos testar a aplicação. Nesse caso, vamos acessar o serviço de frontend e verificar se ele consegue acessar o serviço de backend. Para isso, vamos fazer uma chamada para o endpoint /api/health do serviço de frontend e verificar se ele consegue acessar o serviço de backend. 10.1.config_oeste: kubectl get service/frontend NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE frontend LoadBalancer 10.103.232.28 &lt;external-ip&gt; 8080:30407/TCP 15s curl http://&lt;external-ip&gt;:8080/api/health OK 11. Apagando tudo: Pronto! Agora que você já testou o Skupper, vamos apagar tudo para que você possa testar novamente ou fazer outras experiências. 11.1. Apagando tudo no namespace config_oeste: skupper delete kubectl delete service/frontend kubectl delete deployment/frontend 11.2. Apagando tudo no namespace config_leste: skupper delete kubectl delete deployment/backend Resumo Este exemplo localiza os serviços de front-end e back-end em namespaces diferentes, em clusters diferentes. Normalmente isso significa que eles não tem como se comunicar, a menos que sejam expostos à Internet pública. A introdução do Skupper em cada namespace nos permite criar uma rede de aplicativos virtuais que pode conectar serviços em diferentes clusters. Qualquer serviço exposto na rede de aplicativos é representado como um serviço local em todos os namespaces vinculados. O serviço de back-end está localizado no leste, mas o serviço de front-end no oeste pode “vê-lo” como se fosse local. Quando o front-end envia uma solicitação ao back-end, o Skupper encaminha a solicitação para o namespace em que o back-end está sendo executado e roteia a resposta de volta ao front-end. Não foi necessário expor o serviço de back-end à Internet pública. O Skupper criou uma rede de aplicativos que conecta os serviços em diferentes clusters. O serviço de back-end está localizado no leste, mas o serviço de front-end no oeste pode “vê-lo” como se fosse local. Quando o front-end envia uma solicitação ao back-end, o Skupper encaminha a solicitação para o namespace em que o back-end está sendo executado e roteia a resposta de volta ao front-end. Nenhuma VPN ou conexão Layer 3 foi necessária. O Skupper cria uma rede de aplicativos que conecta os serviços em diferentes clusters. O serviço de back-end está localizado no leste, mas o serviço de front-end no oeste pode “vê-lo” como se fosse local. Quando o front-end envia uma solicitação ao back-end, o Skupper encaminha a solicitação para o namespace em que o back-end está sendo executado e roteia a resposta de volta ao front-end." } ]
